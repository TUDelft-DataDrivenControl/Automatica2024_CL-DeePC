\section{Closed-loop Data-enabled Predictive Control}
This section presents the main result of this article, providing contribution~(\ref{contribution:solves_CL_issue}) whereby we develop \ac{CL-DeePC}. An intuitive explanation is first offered before a more rigorous proof of the method is provided.

As a solution to the identification bias that arises in closed-loop due to correlation between inputs and noise (a demonstration thereof is deferred to Section~\ref{sec:CL_ID_issue}) it is possible to estimate a step-ahead predictor~\citep{Ljung1996}. A prediction horizon length $f>1$ is of more practical use in receding horizon optimal control settings, to which end step-ahead predictors can be applied sequentially. 

Fig.~\ref{fig:CL-DeePC} and \eqref{eq:CL_DeePC_no_IVs} illustrate how this idea is employed in \ac{CL-DeePC}. A step-ahead predictor can be obtained from regular \ac{DeePC} (see Fig.~\ref{fig:regular-DeePC} and \eqref{eq:regular_DeePC_no_IVs}) with $f=1$. In \ac{CL-DeePC} the successive columns of $G$ (from left to right) and their corresponding columns on the right-hand side correspond to sequential applications of regular \ac{DeePC} with $f=1$ to the same matrix of sufficiently persistently exciting past input-output data on the left-hand side as well as time-shifted windows of input-output data on the right-hand side that parameterize successive initial states.
% In this section \ac{CL-DeePC} is first introduced and compared to regular \ac{DeePC}, thereby offering an intuitive explanation for contribution \ref{contribution:solves_CL_issue}. More rigorous explanations follow in subsequent sections. What follows directly below is the principal result of this work.
%
% \begin{figure*}[h!]
%      \centering
%      \begin{subfigure}[b]{\columnwidth}
%          \centering
%          \input{tikzpictures/CL-DeePC}
%          \caption{\ac{CL-DeePC} involves $f$ sequential applications of a step-ahead predictor obtained from regular \ac{DeePC} with $f=1$ (see also Fig.~(b)), resulting in the dashed block-anti diagonals with the same $u_k$ or $y_k$ on the right hand side.}
%          \label{fig:CL-DeePC}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{\columnwidth}
%          \centering
%          \input{tikzpictures/regular_DeePC}
%          \caption{In regular \ac{DeePC} a multi-step ahead predictor of prediction length $f$ is formed directly by taking a single linear combination $g$ of past input-output data. Past data on the right-hand side encodes information on an initial state.}
%         \label{fig:regular-DeePC}
%      \end{subfigure}
%      \caption{Visualization of known (black) and unknown (red) variables in \ac{CL-DeePC} (a) and regular \ac{DeePC} from~\citep{Coulson2019} (b) without \ac{IVs}. Each dot represents an input $u_k\in\mathbb{R}^r$, output $y_k\in\mathbb{R}^l$, or element of a vector $g$.}
% \end{figure*}
\begin{figure}[b!]
\centering
\input{tikzpictures/CL-DeePC}
\caption{Visualization of known (black) and unknown (red) variables in \ac{CL-DeePC} without \ac{IVs}. Each dot represents an input $u_k\in\mathbb{R}^r$, output $y_k\in\mathbb{R}^l$, or element of the matrix $G$. \ac{CL-DeePC} involves $f$ sequential applications of a step-ahead predictor obtained from regular \ac{DeePC} with $f=1$ (see also Fig.~\ref{fig:regular-DeePC}), resulting in the dashed block-anti diagonals with the same $u_k$ or $y_k$ on the right hand side.}
\label{fig:CL-DeePC}
\end{figure}
\begin{figure}[b!]
\centering
\input{tikzpictures/regular_DeePC}
\caption{Visualization of known (black) and unknown (red) variables in regular \ac{DeePC} without \ac{IVs}. Each dot represents an input $u_k\in\mathbb{R}^r$, output $y_k\in\mathbb{R}^l$, or element of the matrix $G$. A multi-step ahead predictor of prediction length $f$ is formed directly by taking a linear combination of past input and output data.\\\vspace{0.75mm}}
\label{fig:regular-DeePC}
\end{figure}
%
\setcounter{thm}{0}
\begin{thm}\label{theorem:main_result}
    Consider the minimal representation of a discrete non-deterministic system given by~\eqref{eqn:SS_innovation}. Let $\bar{u}_k=\left[u_k^\top \; e_k^\top\right]^\top$ represent an `extended' input. Given a sequence of input $u_k$ and output $y_k$ data of length $\bar{N}=N+p$, if the extended input sequence is persistently exciting of order $p+1+n$ and $p\geq\ell$, then the \ac{CL-DeePC} formulation given by~\eqref{eq:CL_DeePC_no_IVs} provides an unbiased output predictor.%asymptotically unbiased output predictor in the limit $N\rightarrow \infty$.
\end{thm}
\textbf{Proof:} 
Rewriting \eqref{eq:DataEq1} with $k=i$ yields
\begin{align*}
    &\underbrace{\begin{bmatrix}-\Gamma_s \tilde{A}^p & -L_s & I_{sl}&-\mathcal{H}_s\end{bmatrix}}_{= \mathfrak{R}}
    \underbrace{\begin{bmatrix}
        X_{i,1,q}\\
        \Phi_{i,s,q}\\
        Y_{i_p,s,q}\\
        E_{i_p,s,q}
    \end{bmatrix}}_{\mathfrak{B}\mathfrak{D}}=\mathcal{O},\\
    &\mathfrak{R}\in\mathbb{R}^{sl\times n+(p+s)(r+l)+sl},\;\mathfrak{BD}\in\mathbb{R}^{n+(p+s)(r+l)+sl \times q},
\end{align*}
with $\mathfrak{R}$, and $\mathfrak{BD}$ as indicated for brevity. Similarly, using \eqref{eq:DataEq1} to decompose $\mathfrak{BD}$ into a matrix $\mathfrak{D}$ of exogenous inputs and initial states and a matrix $\mathfrak{B}$ to describe their effects obtains
\begin{align}
    &\mathfrak{R}
    \underbrace{\begin{bmatrix}
        I_n      & 0      & 0       & 0 & 0\\
        0        & I_{pr} & 0       & 0 & 0\\
        0        & 0      & I_{sr}  & 0 & 0\\
        \Gamma_p & \mathcal{T}_p^\mathrm{u} & 0 & \mathcal{H}_p & 0\\
        \varepsilon_1 & \varepsilon_2 & \mathcal{T}_s^\mathrm{u} & \varepsilon_3 & \mathcal{H}_s\\
        0 & 0 & 0 & 0 & I_{sl}
    \end{bmatrix}}_{=\mathfrak{B}}
    \underbrace{\begin{bmatrix}
        X_{k,1,q}\\
        U_{k,p,q}\\
        U_{k_p,s,q}\\
        E_{k,p,q}\\
        E_{k_p,s,q}
    \end{bmatrix}}_{=\mathfrak{D}}=\mathcal{O},\label{eq:RBD2}\\
    &\mathfrak{B}\in\mathbb{R}^{n+(p+s)(r+l)+sl\times n+(p+s)(r+l)},\notag\\
    &\mathfrak{D}\in\mathbb{R}^{n+(p+s)(r+l)\times q},\notag
\end{align}
with $\varepsilon_1=\Gamma_s(\tilde{A}^p+\tKp{y}\Gamma_p)$, $\varepsilon_2=\Gamma_s(\tKp{u}+\tKp{y}\mathcal{T}_p^\mathrm{u})$, and $\varepsilon_3=\Gamma_s\tKp{y}\mathcal{H}_p$ defined as shown. By inspection, with $\mathcal{R}(\cdot)$ and $\mathcal{N}(\cdot)$ respectively denoting the range and nullspace of a matrix: $\mathcal{R}(\mathfrak{B}\mathfrak{D})\subseteq\mathcal{R}(\mathfrak{B})=\mathcal{N}(\mathfrak{R})$. Only if $\mathfrak{D}$ is full row rank $\mathcal{R}(\mathfrak{B}\mathfrak{D})=\mathcal{N}(\mathfrak{R})$. To see when this latter condition is the case, consider the following.
\setcounter{thm}{0}
\begin{lem}[\cite{Willems2005}, Cor.~2(iii)]
    If the input sequence $\{u_k\}_{k=i}^{i+\epsilon+q-2}$ of a controllable discrete \ac{LTI} system without noise is persistently exciting of order $\epsilon+n$ then the matrix $\left[X_{i,1,q}^\top\;U_{i,\epsilon,q}^\top\right]^\top$ is full row rank.
\end{lem}
This yields the following corollary for extensions to non-deterministic systems, for which we note that controllable $(A,B)$ implies controllable $(A,[B\,K])$.
\setcounter{thm}{0}
\begin{cor}
    If for a non-deterministic \ac{LTI} system of the form given by \eqref{eqn:SS_innovation} with controllable $(A,B)$ the sequence of inputs and noise $\{[u_k^\top\;e_k^\top]^\top\}_{k=i}^{i+\epsilon+q-2}$ is persistently exciting of order $\epsilon+n$ then the matrix $\left[X_{i,1,q}^\top\;U_{i,\epsilon,q}^\top\;E_{i,\epsilon,q}^\top\right]^\top$ is full row rank.
\end{cor}
Hence, $\mathfrak{D}$ in \eqref{eq:RBD2} is full row rank if $\{[u_k^\top\;e_k^\top]^\top\}_{k=i}^{i+p+s+q-2}$ is persistently exciting of order $p+s+n$.

% ==============================================================================================================================================================
% ==============================================================================================================================================================
% \subsection{The basic idea}
% To derive a variant of \ac{DeePC} that does not suffer from the aforementioned closed-loop identification issue lets start by considering regular \ac{DeePC}. Closed-loop identification bias can be avoided by using a step-ahead predictor~\citep{Ljung1996}. However, such a short prediction horizon length is typically not conducive to good performance in a receding horizon control setting. Hence, to obtain another output prediction, the previous regular \ac{DeePC} problem is repeated with the same past data (but a different vector $g$ to span it) to obtain trajectories of input-output data that are shifted forwards one time step. This procedure can be repeated to obtain a desired prediction horizon length $f$. The entire procedure is succinctly described by Fig.~\ref{fig:CL-DeePC} and \eqref{eq:CL_DeePC_no_IVs}, \todo{check length conditions DeePC}

% in which $i$, $i_p$, $\hat{i}$, and $\hat{i}_p$ are discrete time indices (the first three indices lie in the past, and the last index $\hat{i}_p$ resembles the first future time index), and $G$ defines a matrix with columns given by the vectors $\{g_k\}^f_{k=1}$. Furthermore, note that $\Phi_{i,1,N}$ and $\Phi_{\hat{i},1,f}$ are present on respectively the top left and top right hand side.

% Treatment of different \ac{CL-DeePC} solution strategies is deferred to Section \ref{sec:SolutionMethods}. Suffice it for now to take note of the structure of \eqref{eq:CL_DeePC_no_IVs} illustrated by Fig.~\ref{fig:CL-DeePC} and to say that if the input is sufficiently persistently exciting such that $\Phi_{i,1,N}$ is full row rank~\cite[Chapt.~9.6.1]{Verhaegen2007a} then making $\Phi_{i,1,N}$ square and invertible by selecting $N=(p+1)r+pl$ minimizes the number of optimization variables.
% 
% As with regular \ac{DeePC} the idea is to find an optimal combination of allowable future inputs and outputs that minimizes a cost function that is possibly subject to constraints. To see how \eqref{eq:CL_DeePC_no_IVs} can be used in a receding horizon optimal control setting, first consider the top three blocks of the past data matrix. If the input is sufficiently persistently exciting then this matrix, $\Phi_{i,1,N}$, is full row rank~\cite[Chapt.~9.6.1]{Verhaegen2007a}. If, furthermore, ${N=(p+1)r+pl}$, then $\Phi_{i,1,N}$ becomes square and invertible. Hence, a unique solution for $G$ can then be obtained from the top three block equations of \eqref{eq:CL_DeePC_no_IVs} (in terms of future inputs and outputs), which can then be used to obtain output predictions by using the bottom block equation. The structure of \eqref{eq:CL_DeePC_no_IVs} is visualized by Fig.~\ref{fig:CL-DeePC}. This figure demonstrates that successive future output predictions are dependent on preceding input-output data as well as their concurrent input, opening the door to the sequential construction of an output predictor. This is described in \todo{section}, and can be used in a receding horizon optimal control framework.
% 
% 
% ==============================================================================================================================================================
% ==============================================================================================================================================================
% \subsection{The data equations}\label{sec:DerivingDataEquations}
% To motivate a noise mitigation strategy based that is based on \ac{IVs} that is explained hereafter, the data equations that justify this approach are first derived here using a state-space approach.

% To this end, it is straightforward to show by iterative application of respectively \eqref{eqn:SS_innovation} and \eqref{eqn:SS_predictor} that%
% \begin{align}
%     Y_{k_p,s,q} &= \Gamma_s X_{k_p,1,q} + \mathcal{T}_s^\mathrm{u} U_{k_p,s,q} + \mathcal{H}_s E_{k_p,s,q}\label{eq:Yf1},\\
%     \begin{split}%
%     Y_{k_p,s,q} &= \widetilde{\Gamma}_s X_{k_p,1,q} + \widetilde{\mathcal{T}}_s^\mathrm{u} U_{k_p,s,q} + E_{k_p,s,q}\\
%     &\phantom{=}+(I_{sl}-\widetilde{\mathcal{H}}_s)Y_{k_p,s,q}.
%     \end{split}\label{eq:Yf2}
% \end{align}
% It is possible to rewrite the initial states in terms of preceding states and input-output data using \eqref{eqn:SS_predictor} as%
% \begin{align}\label{eq:Xip}
%     X_{k_p,1,q} = \tilde{A}^p X_{k,1,q} + \tKp{u} U_{k,p,q} + \tKp{y} Y_{k,p,q}.
%     % \begin{bmatrix}
%     %     Y_{i,p,q}\\
%     %     U_{i,p,q}
%     % \end{bmatrix}.
% \end{align}
% % in which $\tKp{}=\big[\tKp{y}\;\;\tKp{u}\big]$.
% Substitute \eqref{eq:Xip} into \eqref{eq:Yf1} and \eqref{eq:Yf2} and apply Assumption~\ref{assum:initial_contribution} to obtain two so called data equations:
% \begin{align}
%     Y_{k_p,s,q} &= L_s \Phi_{k,s,q} + \mathcal{H}_s E_{k_p,s,q}\label{eq:DataEq1}\\
%     Y_{k_p,s,q} &= \widetilde{L}_s \Phi_{k,s,q} + E_{k_p,s,q} + (I_{sl}-\widetilde{\mathcal{H}}_s) Y_{k_p,s,q},\label{eq:DataEq2}
% \end{align}
% in which $L_s$, $\widetilde{L}_s$, $\Phi_{k,s,q}$ are defined in Section \ref{sec:notation}. %Similarly to \eqref{eq:DataEq1} and \eqref{eq:DataEq2}, the future outputs are defined by
% % \todo{use noiseless?}%always refer to noiseless version or beter to refer to ideal predictor here?
% % \begin{align}
% %     Y_{\hat{i}_p,s,f} &= L_s \Phi_{\hat{i},s,f} + \mathcal{H}_s E_{\hat{i}_p,s,f},\label{eq:DataEq1.2}\\
% %     Y_{\hat{i}_p,s,f} &= \widetilde{L}_s \Phi_{\hat{i},s,f} + (I_{sl}-\widetilde{\mathcal{H}}_s) Y_{\hat{i}_p,s,f} + E_{\hat{i}_p,s,f}\label{eq:DataEq2.2}.
% % \end{align}
% Although a more generic representation was kept above for later analysis, for \ac{CL-DeePC}, $s=1$. This reduces the complexity of the above equations since ${\widetilde{L}_1=L_1=\big[ C\tKp{u} \; D \; C\tKp{y} \big]}$ and $\widetilde{\mathcal{H}}_1=\mathcal{H}_1=I_l$.
%
% ==============================================================================================================================================================
% ==============================================================================================================================================================
\subsection{Willems' Fundamental Lemma \& Noise}
Equation~\eqref{eq:DataEq1} can be reformulated with $k=i$, $q=N$ or for an ideal noiseless output prediction with $k=\hat{i}$ as respectively
\begin{alignat}{2}
    \begin{bmatrix}
        -L_s & I_{sl}
    \end{bmatrix}&
    \begin{bmatrix}
        \Phi_{i,s,N}\\
        Y_{i_p,s,N}-\mathcal{H}_s E_{i_p,s,N}
    \end{bmatrix} = \mathcal{O},\label{eq:NoisyWFL1}\\%\mathcal{H}_s E_{i_p,s,N}, 
    \begin{bmatrix}
        -L_s & I_{sl}
    \end{bmatrix}&
    \begin{bmatrix}
        \Phi_{\hat{i},s,q}\\
        \widehat{Y}^*_{\hat{i}_p,s,q}
    \end{bmatrix} = \mathcal{O}, \label{eq:NoisyWFL2}
\end{alignat}
in which the asterisk indicates that the output prediction is ideal in the sense of being asymptotically unbiased. Multiplying \eqref{eq:NoisyWFL1} by $\mathcal{Z}^\top G\in\mathbb{R}^{N\times q}$, and subtracting \eqref{eq:NoisyWFL2} obtains
\begin{align}\label{eq:NoisyWFL3}
    \mkern-14mu\begin{bmatrix}
        \shortminus L_s & I_{sl}
    \end{bmatrix}
    \mkern-9mu\left(\mkern-3mu%
    \begin{bmatrix}
        \Phi_{i,s,N}\\
        Y_{i_p,s,N}\shortminus\mathcal{H}_s E_{i_p,s,N}
    \end{bmatrix}%
    \mkern-4mu\mathcal{Z}^\top G%\mkern-2mu
    -%-%
    \mkern-5mu\begin{bmatrix}
        \Phi_{\hat{i},s,q}\\
        \widehat{Y}^*_{\hat{i}_p,s,q}
    \end{bmatrix}\mkern-3mu\right)\mkern-6mu=\mkern-3mu\mathcal{O}\mkern-5mu%\mathcal{H}_s E_{i_p,s,N}\mathcal{Z}G
\end{align}
in which $\mathcal{Z}$ represents a yet unspecified matrix and $G$ represents a matrix that is akin to the likewise defined matrix from \eqref{eq:CL_DeePC_no_IVs} that contains all of the vectors $g_k$.

If the columns of the matrix with data on the left hand side of \eqref{eq:NoisyWFL1} span the entire nullspace of $\left[\shortminus L_s\;I_{sl}\right]$ and $\mathcal{Z}$ is full rank then all solutions to \eqref{eq:NoisyWFL3} are described by equating the term inside the parenthesis to zero. For now, consider the case that $\mathcal{Z}=I_N$, $s=f$, and $q=1$ in the absence of noise to recover the regular deterministic \ac{DeePC} equation~\citep{Coulson2019}. %Then one possible solution (since the matrix $\left[\shortminus L_s\;I_{sl}\right]$ is not full column rank) to \eqref{eq:NoisyWFL3} with $s=f$ and $q=1$ is given by the regular deterministic \ac{DeePC} equation~\cite{Coulson2019}:
\begin{align}\label{eq:regular_DeePC}
    \begin{bmatrix}
        \Phi_{i,f,N}\\
        Y_{i_p,f,N}
    \end{bmatrix}g=%
    \begin{bmatrix}
        \Phi_{\hat{i},f,1}\\
        \widehat{Y}_{\hat{i}_p,f,1}
    \end{bmatrix}.
\end{align}
Willems' Fundamental Lemma makes use of Assumptions~\ref{assum:PE} and~\ref{assum:controllability} to ensure that the entire nullspace of $\left[\shortminus L_f\;I_{fl}\right]$ is spanned by the data matrix on the left hand side of \eqref{eq:regular_DeePC}~\citep{Willems2005}. Assumption~\ref{assum:unique_initial} is furthermore necessary to guarantee the existence of a unique initial state and therefore output predictor. %This clearly reflects Willems' Fundamental Lemma, which states that for a deterministic \ac{LTI} system, any sufficiently persistently exciting past input-output trajectory parameterizes all possible future input-output trajectories~\cite{Willems2005}.\todo{WFL: what about nullspace in (12)}

In the presence of (unknown) noise, the term $Y_{i_p,s,N}-\mathcal{H}_s E_{i_p,s,N}$ from \eqref{eq:NoisyWFL3} cannot be determined to obtain an ideal output predictor. Instead, linear combinations of a noise-corrupted output $Y_{i_p,s,N}$ as in \eqref{eq:regular_DeePC} are taken, resulting in an error of the obtained output predictor due to implicit sampling of $\mathcal{H}_s E_{i_p,s,N}$. Moreover, the regular \ac{DeePC} formulation provided by \eqref{eq:regular_DeePC} may become inconsistent in the presence of noise, prompting the use of, e.g., slack variables and regularization~\citep{Coulson2019}.
%
% ==============================================================================================================================================================
% ==============================================================================================================================================================
\subsection{Noise mitigation using \acl{IVs}}
Notwithstanding potential benefits of beforementioned mechanisms to cope with noise, such methods do not provide a systematic way to mitigate noise at the source. To that end this section introduces the use of an \ac{IV}: $\mathcal{Z}\neq I_N$. In this context, the \ac{IV} is defined such that it is uncorrelated with the noise $E_{i_p,s,N}$ and preserves the (full row) rank of the data matrix $\Phi_{i,s,N}$ obtained from a sufficiently persistently exciting input. These two conditions are respectively formulated as
%
\begin{align}
    &\lim_{N\rightarrow\infty} \frac{1}{N}E_{i_p,s,N}\mathcal{Z}^\top = \mathcal{O},\label{eq:uncorrelated}\\
    \text{rank}\biggl(&\lim_{N\rightarrow\infty} \frac{1}{N}\Phi_{i,s,N}\mathcal{Z}^\top\biggl) =  \text{rank}(\Phi_{i,s,N}),\label{eq:rankconservation}
\end{align}
%
which motivates choosing $\mathcal{Z}=\Phi_{i,s,N}$~\cite[Chapt. 9.6]{Verhaegen2007a}. An important assumption that is hereby introduced to satisfy \eqref{eq:uncorrelated} is that inputs are uncorrelated with noise. To fulfill this assumption Section~\ref{sec:CL_ID_issue} will motivate the choice $s=1$. Furthermore, to then still obtain a multi-step-ahead predictor, $q=f$ is chosen.

Since the noise contribution in \eqref{eq:NoisyWFL3} is then asymptotically attenuated with increasing $N$ this motivates the use of
\begin{align}\label{eq:CL_DeePC_with_IV}
    \begin{bmatrix}
   \Phi_{i,1,N}\Phi_{i,1,N}^\top\\
   \hline
   Y_{i_p,1,N}\Phi_{i,1,N}^\top
    \end{bmatrix}
G =
\begin{bmatrix}
    \Phi_{\hat{i},1,f}\\
    \hline
    \widehat{Y}_{\hat{i}_p,1,f}
\end{bmatrix},
\end{align}
for sufficiently large $N$. Note that the structure of this equation is very similar to \eqref{eq:CL_DeePC_no_IVs} as shown by Fig.~\ref{fig:CL-DeePC}. The main difference is that the matrix with past data on the left hand side loses its indicated block-anti-diagonal structure and has $(p+1)r+pl$ instead of $N$ columns.

Solving \eqref{eq:CL_DeePC_with_IV} for the output predictor using the data equation examplified by \eqref{eq:DataEq1} yields
\begin{align}\label{eq:OutputPredictor}
    \widehat{Y}_{\hat{i}_p,1,f} = L_1 \Phi_{\hat{i},1,f} + \mathcal{H}_1 E_{i_p,1,N}\Phi_{i,1,N}^\dagger\Phi_{\hat{i},1,f},
\end{align}
in which the dagger $\dagger$ denotes the right inverse: ${\Phi_{i,1,N}^\dagger=\Phi_{i,1,N}^\top\left(\Phi_{i,1,N}\Phi_{i,1,N}^\top\right)\inv}$. Similar scrutiny of \eqref{eq:NoisyWFL3} demonstrates that according to \eqref{eq:uncorrelated} the ideal output predictor is recovered from \eqref{eq:OutputPredictor} in the limit $N\rightarrow\infty$.
% In obtaining an output predictor, no systematic noise mitigation strategy is yet applied by \eqref{eq:CL_DeePC_no_IVs} as the columns of $G$ simply take linear combinations of the noise in the output demonstrated by \eqref{eq:DataEq1}. An altered formulation of \eqref{eq:CL_DeePC_no_IVs} is therefore considered that allows the use of \ac{IVs} ($\mathcal{Z}_\mathrm{IV}$) as in~\cite{vanWingerden2022}
% \begin{align}\label{eq:CL_DeePC_with_IV}
%     \begin{bmatrix}
%    \Phi_{i,1,N}\\
%    \hline
%    Y_{i_p,1,N}
%     \end{bmatrix}
% {\mathcal{Z}_\mathrm{IV}}^\top G =
% \begin{bmatrix}
%     \Phi_{\hat{i},1,f}\\
%     \hline
%     \widehat{Y}_{\hat{i}_p,1,f}
% \end{bmatrix},
% \end{align}
% in which $G$ may be different from its previous definition depending on the definition of $\mathcal{Z}_\mathrm{IV}$, which follows shortly. Note that \eqref{eq:CL_DeePC_no_IVs} is recovered with $\mathcal{Z}_\mathrm{IV}=I_N$.

% From \eqref{eq:DataEq1} and \eqref{eq:CL_DeePC_with_IV} the output predictor becomes
% \begin{align}\label{eq:OutputPred}
%     \widehat{Y}_{\hat{i}_p,1,f} = L_1 \Phi_{\hat{i},1,f} + \mathcal{H}_1 E_{i_p,1,N}{\mathcal{Z}_\mathrm{IV}}^\top G.
% \end{align}
% To obtain an output estimate that best resembles a noiseless version of the actual future outputs given by \eqref{eq:DataEq1.2} it is desirable to reduce the noise contribution on the right hand side above. In addition, from an optimization point of view, it would be favorable to choose an \acs{IV} that uniquely determines $G$ from \eqref{eq:CL_DeePC_with_IV} given $\Phi_{\hat{i},1,f}$ and a sufficiently persistently exciting input that ensures that $\Phi_{i,1,N}$ is full row rank.

% This motivates choosing $\mathcal{Z}_\mathrm{IV}=\Phi_{i,1,N}$ as an \acs{IV} since\footnote{Since ${\mathcal{Z}_\mathrm{IV}}^\top G$ is fixed by \eqref{eq:CL_DeePC_IV} scalar multiples of the chosen \ac{IV} would be equally valid, simply resulting in a different $G$.}%
% \begin{align}
%     &\lim_{N\rightarrow\infty} \frac{1}{N}E_{i_p,1,N}{\Phi_{i,1,N}}^\top = \mathcal{O},\\%\label{eq:uncorrelated}\\
%     \text{rank}\biggl(&\lim_{N\rightarrow\infty} \frac{1}{N}\Phi_{i,1,N}{\Phi_{i,1,N}}^\top\biggl) =  \text{rank}(\Phi_{i,1,N}).%\label{eq:rankconservation}
% \end{align}
% As demonstrated by \eqref{eq:uncorrelated}, the instrumental variable and the noise are uncorrelated\footnote{Note that the choice $s=1$ is essential here to avoid correlation between inputs and noise during operation with data obtained in closed-loop.}. Hence, \eqref{eq:OutputPred} asymptotically converges to an ideal, noiseless output predictor with increasing $N$. In addition, provided that the input is sufficiently persistently exciting, $\Phi_{i,1,N}$ is full row rank such that \eqref{eq:rankconservation} permits only a single, unique solution for $G$ in \eqref{eq:CL_DeePC_IV}.