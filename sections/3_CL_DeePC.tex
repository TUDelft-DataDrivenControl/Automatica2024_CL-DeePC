\section{Closed-loop Data-enabled Predictive Control}
This section will first present the new \ac{CL-DeePC} framework, which does not suffer from closed-loop identification bias during operation. Subsequently, data equations that underpin this method are derived upon which a noise mitigation strategy using \ac{IVs} is presented.

% ==============================================================================================================================================================
% ==============================================================================================================================================================
\subsection{The basic idea}
To derive a variant of \ac{DeePC} that does not suffer from the aforementioned closed-loop identification issue lets start by considering regular \ac{DeePC}. As mentioned before, closed-loop identification bias can be avoided by using a step-ahead predictor~\cite{Ljung1996}. However, such a short prediction horizon length is typically not conducive to good performance in a receding horizon control setting. Hence, to obtain another output prediction, lets repeat the previous regular \ac{DeePC} problem with the same past data (but a different vector $g$ to span it) to obtain trajectories of input-output data that are shifted forwards one time step. This procedure can be repeated to obtain a desired prediction horizon length $f$. The entire procedure is succinctly described by Fig.~\ref{fig:CL-DeePC} and \eqref{eq:CL_DeePC_no_IVs}, \todo{check length conditions DeePC}
% 
% To explain the newly-developed \ac{CL-DeePC} framework, consider applying regular \ac{DeePC} $f$ times to the same past data to find consecutive step-ahead predictions of the outputs. This is succinctly described by
% 
% \begin{align}\label{eq:CL_DeePC_no_IVs}
% \begin{bmatrix}
%     U_{i,p,N}\\U_{i_p,1,N}\\Y_{i,p,N}\\ \hline Y_{i_p,1,N}
% \end{bmatrix}
% \underbrace{
% \begin{bmatrix}
%     g_1 & g_2 & \cdots & g_f
% \end{bmatrix}}_{= G} =
% \begin{bmatrix}
%     U_{\hat{i},p,f}\\
%     U_{\hat{i}_p,1,f}\\
%     \widetilde{Y}_{\hat{i},p,f}\\
%     \hline
%     \widehat{Y}_{\hat{i}_p,1,f}
% \end{bmatrix},
% % \begin{bmatrix}
% %     \datavec{u}{\hat{i},p} & \datavec{u}{\hat{i}+1,p} & \cdots & \datavec{u}{\hat{i}+f-1,p}\\
% %     \datavec{u}{\hat{i}_p,1} & \datavec{u}{\hat{i}_p+1,1} & \cdots & \datavec{u}{\hat{i}_p+f-1,1}\\
% %     \datavec{y}{\hat{i},p} & \datavec{y}{\hat{i}+1,p} & \cdots & \datavec{y}{\hat{i}+f-1,p}\\
% %     \datavec{y}{\hat{i}_p,1} & \datavec{y}{\hat{i}_p+1,1} & \cdots & \datavec{y}{\hat{i}_p+f-1,1}
% % \end{bmatrix}
% \end{align}
in which $i$, $i_p$, $\hat{i}$, and $\hat{i}_p$ are discrete time indices (the first three indices lie in the past, and the last index $\hat{i}_p$ resembles the first future time index), and $G$ defines a matrix with columns given by the vectors $\{g_k\}^f_{k=1}$. Furthermore, note that $\mathcal{Z}_{i,1,N}$ and $\mathcal{Z}_{\hat{i},1,f}$ are present on respectively the top left and top right hand side.

Treatment of different \ac{CL-DeePC} solution strategies is deferred to Section \ref{sec:SolutionMethods}. Suffice it for now to take note of the structure of \eqref{eq:CL_DeePC_no_IVs} illustrated by Fig.~\ref{fig:CL-DeePC} and to say that if the input is sufficiently persistently exciting such that $\mathcal{Z}_{i,1,N}$ is full row rank~\cite[Chapt.~9.6.1]{Verhaegen2007a} then making $\mathcal{Z}_{i,1,N}$ square and invertible by selecting $N=(p+1)r+pl$ minimizes the number of optimization variables.
% 
% As with regular \ac{DeePC} the idea is to find an optimal combination of allowable future inputs and outputs that minimizes a cost function that is possibly subject to constraints. To see how \eqref{eq:CL_DeePC_no_IVs} can be used in a receding horizon optimal control setting, first consider the top three blocks of the past data matrix. If the input is sufficiently persistently exciting then this matrix, $\mathcal{Z}_{i,1,N}$, is full row rank~\cite[Chapt.~9.6.1]{Verhaegen2007a}. If, furthermore, ${N=(p+1)r+pl}$, then $\mathcal{Z}_{i,1,N}$ becomes square and invertible. Hence, a unique solution for $G$ can then be obtained from the top three block equations of \eqref{eq:CL_DeePC_no_IVs} (in terms of future inputs and outputs), which can then be used to obtain output predictions by using the bottom block equation. The structure of \eqref{eq:CL_DeePC_no_IVs} is visualized by Fig.~\ref{fig:CL-DeePC}. This figure demonstrates that successive future output predictions are dependent on preceding input-output data as well as their concurrent input, opening the door to the sequential construction of an output predictor. This is described in \todo{section}, and can be used in a receding horizon optimal control framework.
% 
\begin{figure}[t!]
\centering
\input{tikzpictures/CL-DeePC}
\caption{Visualization of known (black) and unknown (red) variables in \ac{CL-DeePC} without \ac{IVs}. Each dot represents an input $u_k\in\mathbb{R}^r$, output $y_k\in\mathbb{R}^l$, or element of the matrix $G$. Dashed block-anti-diagonals are composed of the same $u_k$ or $y_k$.}
%Note that dashed block-anti-diagonals of inputs or outputs are composed of the same time sample.} %Without \ac{IVs}, as in \eqref{eq:CL_DeePC_no_IVs}, $m=N$, whilst with \ac{IVs}, as in \eqref{eq:CL_DeePC_IV}, $m=(p+1)r+pl$.}
\label{fig:CL-DeePC}
\end{figure}

% ==============================================================================================================================================================
% ==============================================================================================================================================================
\subsection{The data equations}\label{sec:DerivingDataEquations}
To motivate a noise mitigation strategy based that is based on \ac{IVs} that is explained hereafter, the data equations that justify this approach are first derived here using a state-space approach.

To this end, it is straightforward to show by iterative application of respectively \eqref{eqn:SS_innovation} and \eqref{eqn:SS_predictor} that%
\begin{align}
    Y_{k_p,s,q} &= \Gamma_s X_{k_p,1,q} + \mathcal{T}_s^\mathrm{u} U_{k_p,s,q} + \mathcal{H}_s E_{k_p,s,q}\label{eq:Yf1},\\
    \begin{split}%
    Y_{k_p,s,q} &= \widetilde{\Gamma}_s X_{k_p,1,q} + \widetilde{\mathcal{T}}_s^\mathrm{u} U_{k_p,s,q} + E_{k_p,s,q}\\
    &\phantom{=}+(I_{sl}-\widetilde{\mathcal{H}}_s)Y_{k_p,s,q},
    \end{split}\label{eq:Yf2}
\end{align}
in which $s\in\mathbb{Z}^+$ represents a window length, and $q\in\mathbb{Z}^+$ represents the number of block-Hankel matrix columns. It is possible to rewrite the initial states in terms of preceding states and input-output data using \eqref{eqn:SS_predictor} as%
\begin{align}\label{eq:Xip}
    X_{k_p,1,q} = \tilde{A}^p X_{k,1,q} + \tKp{u} U_{k,p,q} + \tKp{y} Y_{k,p,q}.
    % \begin{bmatrix}
    %     Y_{i,p,q}\\
    %     U_{i,p,q}
    % \end{bmatrix}.
\end{align}
% in which $\tKp{}=\big[\tKp{y}\;\;\tKp{u}\big]$.
Substitute \eqref{eq:Xip} into \eqref{eq:Yf1} and \eqref{eq:Yf2} and apply Assumption~\ref{assum:initial_contribution} to obtain two so called data equations:
\begin{align}
    Y_{k_p,s,q} &= L_s \mathcal{Z}_{k,s,q} + \mathcal{H}_s E_{k_p,s,q}\label{eq:DataEq1}\\
    Y_{k_p,s,q} &= \widetilde{L}_s \mathcal{Z}_{k,s,q} + (I_{sl}-\widetilde{\mathcal{H}}_s) Y_{k_p,s,q} + E_{k_p,s,q},\label{eq:DataEq2}
\end{align}
in which $L_s$, $\widetilde{L}_s$, $\mathcal{Z}_{k,s,q}$ are defined in Section \ref{sec:notation}. %Similarly to \eqref{eq:DataEq1} and \eqref{eq:DataEq2}, the future outputs are defined by
% \todo{use noiseless?}%always refer to noiseless version or beter to refer to ideal predictor here?
% \begin{align}
%     Y_{\hat{i}_p,s,f} &= L_s \mathcal{Z}_{\hat{i},s,f} + \mathcal{H}_s E_{\hat{i}_p,s,f},\label{eq:DataEq1.2}\\
%     Y_{\hat{i}_p,s,f} &= \widetilde{L}_s \mathcal{Z}_{\hat{i},s,f} + (I_{sl}-\widetilde{\mathcal{H}}_s) Y_{\hat{i}_p,s,f} + E_{\hat{i}_p,s,f}\label{eq:DataEq2.2}.
% \end{align}
Although a more generic representation was kept above for later analysis, for \ac{CL-DeePC}, $s=1$. This reduces the complexity of the above equations since ${\widetilde{L}_1=L_1=\big[ C\tKp{u} \; D \; C\tKp{y} \big]}$ and $\widetilde{\mathcal{H}}_1=\mathcal{H}_1=I_l$.
%
% ==============================================================================================================================================================
% ==============================================================================================================================================================
\subsection{Willems' Fundamental Lemma \& Noise}
Equation~\eqref{eq:DataEq1} can be reformulated with $k=i$, $q=N$ or for an ideal noiseless output prediction with $k=\hat{i}$ as respectively
\begin{alignat}{2}
    \begin{bmatrix}
        -L_s & I_{sl}
    \end{bmatrix}&
    \begin{bmatrix}
        \mathcal{Z}_{i,s,N}\\
        Y_{i_p,s,N}-\mathcal{H}_s E_{i_p,s,N}
    \end{bmatrix} = \mathcal{O},\label{eq:NoisyWFL1}\\%\mathcal{H}_s E_{i_p,s,N}, 
    \begin{bmatrix}
        -L_s & I_{sl}
    \end{bmatrix}&
    \begin{bmatrix}
        \mathcal{Z}_{\hat{i},s,q}\\
        \widehat{Y}^*_{\hat{i}_p,s,q}
    \end{bmatrix} = \mathcal{O}, \label{eq:NoisyWFL2}
\end{alignat}
in which the asterisk indicates that the output prediction is ideal in the sense of being asymptotically unbiased. Multiplying \eqref{eq:NoisyWFL1} by $\mathcal{I}G\in\mathbb{R}^{N\times q}$, and subtracting \eqref{eq:NoisyWFL2} obtains
\begin{align}\label{eq:NoisyWFL3}
    \mkern-14mu\begin{bmatrix}
        \shortminus L_s & I_{sl}
    \end{bmatrix}
    \mkern-9mu\left(\mkern-3mu%
    \begin{bmatrix}
        \mathcal{Z}_{i,s,N}\\
        Y_{i_p,s,N}\shortminus\mathcal{H}_s E_{i_p,s,N}
    \end{bmatrix}%
    \mkern-4mu\mathcal{I}G%\mkern-2mu
    -%-%
    \mkern-5mu\begin{bmatrix}
        \mathcal{Z}_{\hat{i},s,q}\\
        \widehat{Y}^*_{\hat{i}_p,s,q}
    \end{bmatrix}\mkern-3mu\right)\mkern-6mu=\mkern-3mu\mathcal{O}\mkern-5mu%\mathcal{H}_s E_{i_p,s,N}\mathcal{I}G
\end{align}
in which $\mathcal{I}$ represents a yet unspecified matrix and $G$ represents a matrix that is akin to the likewise defined matrix from \eqref{eq:CL_DeePC_no_IVs} that contains all of the vectors $g_k$.

If the columns of the matrix with data on the left hand side of \eqref{eq:NoisyWFL1} spans the entire nullspace of $\left[\shortminus L_s\;I_{sl}\right]$ and $\mathcal{I}$ is full rank then all solutions to \eqref{eq:NoisyWFL3} are described by equating the term inside the parenthesis to zero. For now, consider the case that $\mathcal{I}=I_N$, $s=f$, and $q=1$ in the absence of noise to recover the regular deterministic \ac{DeePC} equation~\citep{Coulson2019}. %Then one possible solution (since the matrix $\left[\shortminus L_s\;I_{sl}\right]$ is not full column rank) to \eqref{eq:NoisyWFL3} with $s=f$ and $q=1$ is given by the regular deterministic \ac{DeePC} equation~\cite{Coulson2019}:
\begin{align}\label{eq:regular_DeePC}
    \begin{bmatrix}
        \mathcal{Z}_{i,f,N}\\
        Y_{i_p,f,N}
    \end{bmatrix}g=%
    \begin{bmatrix}
        \mathcal{Z}_{\hat{i},f,1}\\
        \widehat{Y}_{\hat{i}_p,f,1}
    \end{bmatrix}.
\end{align}
Willems' Fundamental Lemma makes use of Assumptions~\ref{assum:PE} and~\ref{assum:controllability} to ensure that the entire nullspace of $\left[\shortminus L_f\;I_{fl}\right]$ is spanned by the data matrix on the left hand side of \eqref{eq:regular_DeePC}~\citep{Willems2005}. Assumption~\ref{assum:unique_initial} is furthermore necessary to guarantee the existence of a unique initial state and therefore output predictor~\cite{}. %This clearly reflects Willems' Fundamental Lemma, which states that for a deterministic \ac{LTI} system, any sufficiently persistently exciting past input-output trajectory parameterizes all possible future input-output trajectories~\cite{Willems2005}.\todo{WFL: what about nullspace in (12)}

However, in the presence of (unknown) noise, the term $Y_{i_p,s,N}-\mathcal{H}_s E_{i_p,s,N}$ from \eqref{eq:NoisyWFL3} cannot be determined to obtain an ideal output predictor. Instead, linear combinations of a noise-corrupted output $Y_{i_p,s,N}$ as in \eqref{eq:regular_DeePC} are taken, resulting in an error of the obtained output predictor due to implicit sampling of $\mathcal{H}_s E_{i_p,s,N}$. Moreover, the regular \ac{DeePC} formulation provided by \eqref{eq:regular_DeePC} may become inconsistent in the presence of noise, prompting the use of, e.g., slack variables and regularization~\cite{Coulson2019}.
%
% ==============================================================================================================================================================
% ==============================================================================================================================================================
\subsection{Noise mitigation using \acl{IVs}}
Notwithstanding potential benefits of beforementioned mechanisms to cope with noise, such methods do not provide a systematic way to mitigate noise at the source. To that end this section introduces the use of an \ac{IV}: $\mathcal{I}\neq I_N$. In this context, the \ac{IV} is defined such that it is uncorrelated with the noise $E_{i_p,s,N}$ and preserves the (full row) rank of the data matrix $\mathcal{Z}_{i,s,N}$ obtained from a sufficiently persistently exciting input. These two conditions are respectively formulated as
%
\begin{align}
    &\lim_{N\rightarrow\infty} \frac{1}{N}E_{i_p,s,N}\mathcal{I}^\top = \mathcal{O},\label{eq:uncorrelated}\\
    \text{rank}\biggl(&\lim_{N\rightarrow\infty} \frac{1}{N}\mathcal{Z}_{i,s,N}\mathcal{I}^\top\biggl) =  \text{rank}(\mathcal{Z}_{i,s,N}),\label{eq:rankconservation}
\end{align}
%
which motivates choosing $\mathcal{I}=\mathcal{Z}_{i,s,N}$~\cite[Chapt. 9.6]{Verhaegen2007a}. An important assumption that is hereby introduced to satisfy \eqref{eq:uncorrelated} is that inputs are uncorrelated with noise. To fulfill this assumption Section~\ref{sec:CL_ID_issue} will motivate the choice $s=1$. Furthermore, to then still obtain a multi-step-ahead predictor, $q=f$ is chosen.

Since the noise contribution in \eqref{eq:NoisyWFL3} is then asymptotically attenuated with increasing $N$ this motivates the use of
\begin{align}\label{eq:CL_DeePC_with_IV}
    \begin{bmatrix}
   \mathcal{Z}_{i,1,N}\mathcal{Z}_{i,1,N}^\top\\
   \hline
   Y_{i_p,1,N}\mathcal{Z}_{i,1,N}^\top
    \end{bmatrix}
G =
\begin{bmatrix}
    \mathcal{Z}_{\hat{i},1,f}\\
    \hline
    \widehat{Y}_{\hat{i}_p,1,f}
\end{bmatrix},
\end{align}
for sufficiently large $N$. Note that the structure of this equation is very similar to \eqref{eq:CL_DeePC_no_IVs} as shown by Fig.~\ref{fig:CL-DeePC}. The main difference is that the matrix with past data on the left hand side loses its indicated block-anti-diagonal structure and has $(p+1)r+pl$ instead of $N$ columns.

Solving \eqref{eq:CL_DeePC_with_IV} for the output predictor using the data equation examplified by \eqref{eq:DataEq1} yields
\begin{align}\label{eq:OutputPredictor}
    \widehat{Y}_{\hat{i}_p,1,f} = L_1 \mathcal{Z}_{\hat{i},1,f} + \mathcal{H}_1 E_{i_p,1,N}\mathcal{Z}_{i,1,N}^\dagger\mathcal{Z}_{\hat{i},1,f},
\end{align}
in which the dagger $\dagger$ denotes the right inverse: ${\mathcal{Z}_{i,1,N}^\dagger=\mathcal{Z}_{i,1,N}^\top\left(\mathcal{Z}_{i,1,N}\mathcal{Z}_{i,1,N}^\top\right)\inv}$. Similar scrutiny of \eqref{eq:NoisyWFL3} demonstrates that according to \eqref{eq:uncorrelated} the ideal output predictor is recovered from \eqref{eq:OutputPredictor} in the limit $N\rightarrow\infty$.
% In obtaining an output predictor, no systematic noise mitigation strategy is yet applied by \eqref{eq:CL_DeePC_no_IVs} as the columns of $G$ simply take linear combinations of the noise in the output demonstrated by \eqref{eq:DataEq1}. An altered formulation of \eqref{eq:CL_DeePC_no_IVs} is therefore considered that allows the use of \ac{IVs} ($\mathcal{I}_\mathrm{IV}$) as in~\cite{vanWingerden2022}
% \begin{align}\label{eq:CL_DeePC_with_IV}
%     \begin{bmatrix}
%    \mathcal{Z}_{i,1,N}\\
%    \hline
%    Y_{i_p,1,N}
%     \end{bmatrix}
% {\mathcal{I}_\mathrm{IV}}^\top G =
% \begin{bmatrix}
%     \mathcal{Z}_{\hat{i},1,f}\\
%     \hline
%     \widehat{Y}_{\hat{i}_p,1,f}
% \end{bmatrix},
% \end{align}
% in which $G$ may be different from its previous definition depending on the definition of $\mathcal{I}_\mathrm{IV}$, which follows shortly. Note that \eqref{eq:CL_DeePC_no_IVs} is recovered with $\mathcal{I}_\mathrm{IV}=I_N$.

% From \eqref{eq:DataEq1} and \eqref{eq:CL_DeePC_with_IV} the output predictor becomes
% \begin{align}\label{eq:OutputPred}
%     \widehat{Y}_{\hat{i}_p,1,f} = L_1 \mathcal{Z}_{\hat{i},1,f} + \mathcal{H}_1 E_{i_p,1,N}{\mathcal{I}_\mathrm{IV}}^\top G.
% \end{align}
% To obtain an output estimate that best resembles a noiseless version of the actual future outputs given by \eqref{eq:DataEq1.2} it is desirable to reduce the noise contribution on the right hand side above. In addition, from an optimization point of view, it would be favorable to choose an \acs{IV} that uniquely determines $G$ from \eqref{eq:CL_DeePC_with_IV} given $\mathcal{Z}_{\hat{i},1,f}$ and a sufficiently persistently exciting input that ensures that $\mathcal{Z}_{i,1,N}$ is full row rank.

% This motivates choosing $\mathcal{I}_\mathrm{IV}=\mathcal{Z}_{i,1,N}$ as an \acs{IV} since\footnote{Since ${\mathcal{I}_\mathrm{IV}}^\top G$ is fixed by \eqref{eq:CL_DeePC_IV} scalar multiples of the chosen \ac{IV} would be equally valid, simply resulting in a different $G$.}%
% \begin{align}
%     &\lim_{N\rightarrow\infty} \frac{1}{N}E_{i_p,1,N}{\mathcal{Z}_{i,1,N}}^\top = \mathcal{O},\\%\label{eq:uncorrelated}\\
%     \text{rank}\biggl(&\lim_{N\rightarrow\infty} \frac{1}{N}\mathcal{Z}_{i,1,N}{\mathcal{Z}_{i,1,N}}^\top\biggl) =  \text{rank}(\mathcal{Z}_{i,1,N}).%\label{eq:rankconservation}
% \end{align}
% As demonstrated by \eqref{eq:uncorrelated}, the instrumental variable and the noise are uncorrelated\footnote{Note that the choice $s=1$ is essential here to avoid correlation between inputs and noise during operation with data obtained in closed-loop.}. Hence, \eqref{eq:OutputPred} asymptotically converges to an ideal, noiseless output predictor with increasing $N$. In addition, provided that the input is sufficiently persistently exciting, $\mathcal{Z}_{i,1,N}$ is full row rank such that \eqref{eq:rankconservation} permits only a single, unique solution for $G$ in \eqref{eq:CL_DeePC_IV}.