\section{Efficient Implementation of \acs{CL-DeePC}}\label{sec:Sequential}
Having demonstrated the consistency of \ac{CL-DeePC} in the previous section, this section presents an efficient implementation method w.r.t. the number of independent optimization variables.

The use of such an efficient method can be understood by comparing the number of independent optimization variables (in red) in respectively Fig.~\ref{fig:CL-DeePC} and Fig.~\ref{fig:regular-DeePC}. Both with and without \ac{IVs}, the number of such independent optimization variables used in \ac{CL-DeePC} scales worse with $f$ than is the case for \ac{DeePC}. This is inherent to the formulation depicted by Fig.~\ref{fig:CL-DeePC} whereby multiple vector.

Alternatively, one may exploit the sequential nature of \ac{CL-DeePC} as shown by the columns of \eqref{eq:Yfhat_1}. For $k\in[\hat{i}_p,\hat{i}_p+f)$ this obtains
\begin{align}\label{eq:yhat_k}
    \hat{y}_{k} = \begin{bmatrix} \tilde{\beta}_1 & \cdots & \tilde{\beta}_{p+1} \end{bmatrix} \datavec{u}{k-p,p+1} + \begin{bmatrix} \tilde{\theta}_1 & \cdots & \tilde{\theta}_{p} \end{bmatrix} \datavec{\overline{y}}{k-p,p},% \qquad \forall k\in[\hat{i}_p,\hat{i}_p+f)
\end{align}
in which $\tilde{\beta}_{(\cdot)}\in\mathbb{R}^{l\times r}$, and $\tilde{\theta}_{(\cdot)}\in\mathbb{R}^{l\times l}$ are determined from $Y_{i_p,1,f}\mathcal{Z}^\top\left(\Psi_{i,1,N}\mathcal{Z}^\top\right)\inv$ in \eqref{eq:Yfhat_1}. Sequential application of the step-ahead predictor \eqref{eq:yhat_k} leads to
\begin{align}\label{eq:Sequential1}
    \datavec{\hat{y}}{\hat{i}_p,f} &=
    \begin{bmatrix}
        \widetilde{\mathcal{L}}^\mathrm{u}_f & \widetilde{\mathcal{G}}^\mathrm{u}_f 
    \end{bmatrix}    
    \begin{bmatrix}
        \datavec{u}{\hat{i},p}\\
        \datavec{u}{\hat{i}_p,f}
    \end{bmatrix}+
    \begin{bmatrix}
        \widetilde{\mathcal{L}}^\mathrm{y}_f & \widetilde{\mathcal{G}}^\mathrm{y}_f 
    \end{bmatrix}    
    \begin{bmatrix}
        \datavec{y}{\hat{i},p}\\
        \datavec{\hat{y}}{\hat{i}_p,f}
    \end{bmatrix},
\end{align}
in which
\begin{align*}
    % -----------------------------------------------------------------------------------------------------------------
    \begin{bmatrix}
        \widetilde{\mathcal{L}}^\mathrm{u}_f & \widetilde{\mathcal{G}}^\mathrm{u}_f 
    \end{bmatrix}&= {\scriptsize
    \begin{bmatrix}
        \tilde{\beta}_1     & \cdots      & \tilde{\beta}_{p}   & \tilde{\beta}_{p+1} & 0           & 0           & \cdots      & 0          \\
        0           & \tilde{\beta}_1     & \cdots      & \tilde{\beta}_{p}   & \tilde{\beta}_{p+1} & 0           & \cdots      & 0          \\
        \vdots      & \ddots      & \ddots      &             & \ddots      & \ddots      & \ddots      & \vdots     \\
        0           & \cdots      & 0           & \tilde{\beta}_1     & \cdots      & \tilde{\beta}_{p}   & \tilde{\beta}_{p+1} & 0          \\
        0           & \cdots      & 0           & 0           & \tilde{\beta}_1     & \cdots      & \tilde{\beta}_{p}   & \tilde{\beta}_{p+1}\\
    \end{bmatrix}},\\
    % -----------------------------------------------------------------------------------------------------------------
    \begin{bmatrix}
        \widetilde{\mathcal{L}}^\mathrm{y}_f & \widetilde{\mathcal{G}}^\mathrm{y}_f 
    \end{bmatrix}&= {\scriptsize
    \begin{bmatrix}
        \tilde{\theta}_1    & \cdots      & \tilde{\theta}_{p}  & 0            & 0            & 0           & \cdots       & 0          \\
        0           & \tilde{\theta}_1    & \cdots      & \tilde{\theta}_{p}   & 0            & 0           & \cdots       & 0          \\
        \vdots      & \ddots      & \ddots      &              & \ddots       & \ddots      & \ddots       & \vdots     \\
        0           & \cdots      & 0           & \tilde{\theta}_1     & \cdots       & \tilde{\theta}_{p}  & 0            & 0          \\
        0           & \cdots      & 0           & 0            & \tilde{\theta}_1     & \cdots      & \tilde{\theta}_{p}   & 0          \\
    \end{bmatrix}},
\end{align*}
with ${\widetilde{\mathcal{L}}^\mathrm{u}_f\in\mathbb{R}^{fl\times pr}}$, ${\widetilde{\mathcal{G}}^\mathrm{u}_f\in\mathbb{R}^{fl\times fr}}$, ${\widetilde{\mathcal{L}}^\mathrm{y}_f\in\mathbb{R}^{fl\times pl}}$, and ${\widetilde{\mathcal{G}}^\mathrm{y}_f\in\mathbb{R}^{fl\times fl}}$. The subscript of these matrices indicates the number of block rows.

Notice that the predicted future outputs feature on both sides of \eqref{eq:Sequential1}. Solving for these predicted outputs yields
\begin{align}\label{eq:Sequential2}
    \datavec{\hat{y}}{\hat{i}_p,f} &=
    \begin{bmatrix}
        \mathcal{L}^\mathrm{u}_f & \mathcal{L}^\mathrm{y}_f 
    \end{bmatrix}    
    \begin{bmatrix}
        \datavec{u}{\hat{i},p}\\
        \datavec{y}{\hat{i},p}
    \end{bmatrix}+
    \mathcal{G}^\mathrm{u}_f
    \datavec{u}{\hat{i}_p,f},
\end{align}
in which $\mathcal{L}^\mathrm{u}_f$, $\mathcal{G}^\mathrm{u}_f$, and $\mathcal{L}^\mathrm{y}_f$ are uniquely defined by
\begin{align}\label{eq:Sequential3}
    \left(I_{fl}-\widetilde{\mathcal{G}}^\mathrm{y}_f\right)
    \begin{bmatrix}
        \mathcal{L}^\mathrm{u}_f & \mathcal{G}^\mathrm{u}_f & \mathcal{L}^\mathrm{y}_f
    \end{bmatrix}=
    % \left(I_{fl}-\widetilde{\mathcal{G}}^\mathrm{y}_f\right)\inv
    \begin{bmatrix}
        \widetilde{\mathcal{L}}^\mathrm{u}_f & \widetilde{\mathcal{G}}^\mathrm{u}_f & \widetilde{\mathcal{L}}^\mathrm{y}_f
    \end{bmatrix}.
\end{align}
Since $I_{fl}-\widetilde{\mathcal{G}}^\mathrm{y}_f$ is invertible, \eqref{eq:Sequential3} can be solved directly for $\big[\mathcal{L}^\mathrm{u}_f \; \mathcal{G}^\mathrm{u}_f \; \mathcal{L}^\mathrm{y}_f\big]$ to construct the predictor \eqref{eq:Sequential2}. Alternatively, an efficient sequential procedure is also possible to solve \eqref{eq:Sequential3} that exploits the structure of $I_{fl}-\widetilde{\mathcal{G}}^\mathrm{y}_f$.

For this sequential procedure, define the $f$ block-rows of $\big[\widetilde{\mathcal{L}}^\mathrm{u}_f \; \widetilde{\mathcal{G}}^\mathrm{u}_f \; \widetilde{\mathcal{L}}^\mathrm{y}_f\big]$ and $\big[\mathcal{L}^\mathrm{u}_f \; \mathcal{G}^\mathrm{u}_f \; \mathcal{L}^\mathrm{y}_f\big]$ by respectively $\tilde{\alpha}_j$, ${\alpha_j\in\mathbb{R}^{l\times p(r+l)+fr}}$, with $j$ here representing the index of the block row: $j=0,1,\dots,f-1$. It is then straightforward to show from \eqref{eq:Sequential3} that the formulation
\begin{align}\label{eq:Sequential4}
    \alpha_j=
    \left\{\begin{array}{ll}
    0          ,     & \text{if } j<0\\
    \tilde{\alpha}_j,& \text{if } j=0\\
    \tilde{\alpha}_j + \sum\limits_{r=1}^{p}\tilde{\theta}_r\alpha_{r-p+j-1}, & \text{if } j \geq 1
    \end{array}\right.
\end{align}
 allows efficient sequential construction of $\big[\mathcal{L}^\mathrm{u}_f \; \mathcal{G}^\mathrm{u}_f \; \mathcal{L}^\mathrm{y}_f\big]$ starting from $j=0$. From the subsequent section, it will become clear that $\mathcal{G}^\mathrm{u}_f$ is a causal block-Toeplitz matrix, meaning that the matrix is fully parameterized by its leftmost block column. As such one may choose to only solve this portion of $\mathcal{G}^\mathrm{u}_f$ in \eqref{eq:Sequential3} using \eqref{eq:Sequential4} and to complete the matrix $\mathcal{G}^\mathrm{u}_f$ thereafter.