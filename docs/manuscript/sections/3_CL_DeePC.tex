\section{Closed-loop Data-enabled Predictive Control}
This section presents the main result of this article, providing contribution~(\ref{contribution:solves_CL_issue}) whereby we develop \ac{CL-DeePC}. An intuitive explanation is first offered before a proof of the underlying main result is provided.

As a solution to the identification bias that arises in closed-loop due to correlation between inputs and noise (a demonstration thereof is deferred to Section~\ref{sec:CL_ID_issue}) it is possible to estimate a step-ahead predictor~\citep{Ljung1996}. A prediction horizon length $f>1$ is of more practical use in receding horizon optimal control settings, to which end step-ahead predictors can be applied sequentially. 

Fig.~\ref{fig:CL-DeePC} and \eqref{eq:CL_DeePC_no_IVs} illustrate how this idea is employed in \ac{CL-DeePC}. A step-ahead predictor can be obtained from \ac{DeePC} (see Fig.~\ref{fig:regular-DeePC} and \eqref{eq:regular_DeePC_no_IVs}) with $f=1$. In \ac{CL-DeePC} the successive columns of $G$ (from left to right) and their corresponding columns on the right-hand side correspond to sequential applications of \ac{DeePC} with $f=1$ to the same matrix of sufficiently persistently exciting past input-output data on the left-hand side as well as time-shifted windows of input-output data on the right-hand side that encode information on successive initial states.\\ 
%
\begin{figure}[b!]
\centering
\input{tikzpictures/CL-DeePC}
\caption{Visualization of known (black) and unknown (red) variables in \ac{CL-DeePC} without \ac{IVs}. Each dot represents an input $u_k\in\mathbb{R}^r$, output $y_k\in\mathbb{R}^l$, or element of the matrix $G$. \ac{CL-DeePC} involves $f$ sequential applications of a step-ahead predictor obtained from regular \ac{DeePC} with $f=1$ (see also Fig.~\ref{fig:regular-DeePC}), resulting in the dashed block-anti diagonals with the same $u_k$ or $y_k$ on the right hand side.}
\label{fig:CL-DeePC}
\end{figure}
\begin{figure}[b!]
\centering
\input{tikzpictures/regular_DeePC}
\caption{Visualization of known (black) and unknown (red) variables in \ac{DeePC} without \ac{IVs}. Each dot represents an input $u_k\in\mathbb{R}^r$, output $y_k\in\mathbb{R}^l$, or element of the matrix $G$. A multi-step ahead predictor of prediction length $f$ is formed directly by taking a linear combination of past input and output data.\\\vspace{0.75mm}}
\label{fig:regular-DeePC}
\end{figure}
%
\setcounter{thm}{0}
\begin{thm}\label{theorem:main_result}
    Consider the minimal discrete non-deterministic \ac{LTI} system given by~\eqref{eqn:SS_innovation} to generate input-output data in closed-loop by means of a causal controller without direct feedthrough, and data matrices $\overline{\Psi}_{\hat{i},1,f}$ and $\Psi_{i,1,N_\mathrm{s}}$ as in \eqref{eq:Phi_def}. If $N_\mathrm{s}=(p+1)r+pl$ such that $\Psi_{i,1,N_\mathrm{s}}$ is square, and furthermore, if $\Psi_{i,1,N_\mathrm{s}}$ is invertible, %
    %
    % If the joint input and noise sequences are sufficiently persistently exciting such that $\left[X_{i,1,N}^\top \; U_{i,p,N}^\top \; U_{i_p,1,N}^\top \; E_{i,p,N}^\top\right]^\top$ is full row rank %
    % ------------------------------------------------------
    %% ----------------- old version below -----------------
    %If the input sequence $\{u_k\}_{k=i}^{i+\bar{N}-1}$ of length $\bar{N}=p+s+N-1$ %, with $N\geq(p+s+n)(r+l)+n$ and $p\geq\ell$\todo{don't forget},
    % is persistently exciting of order $p+s+n$, and has sample correlations such that%
    % \begin{alignat}{2}%see also https://www.cis.upenn.edu/~jean/schur-comp.pdf
    % % \widehat{\Sigma}_{u,u} &> 0,\label{eq:PE_corU}\\
    % &\widehat{\Sigma}_{\mathrm{ee}} - \widehat{\Sigma}_{\mathrm{ue}^\top} \widehat{\Sigma}_{\mathrm{uu}}\inv \widehat{\Sigma}_{\mathrm{ue}}\succ0,\span\span\label{eq:PE_corUE2}\\
    % &&\text{with}\quad\widehat{\Sigma}_{\mathrm{ee}}&=E_{i,p+s+n,N-n}E_{i,p+s+n,N-n}^\top,\notag\\
    % &&\widehat{\Sigma}_{\mathrm{ue}}&=U_{i,p+s+n,N-n}E_{i,p+s+n,N-n}^\top,\notag\\
    % &&\widehat{\Sigma}_{\mathrm{uu}}&=U_{i,p+s+n,N-n}U_{i,p+s+n,N-n}^\top,\notag
    % \end{alignat}
    % ------------------------------------------------------
    then \\
    $\mathrm{(i)}$ $\exists G\in\mathbb{R}^{N\times f}$ such that
    \begin{align}\tag{\ref{eq:CL_DeePC_no_IVs}}%\label{eq:Theorem1}
        \begin{bmatrix}
            \Psi_{i,1,N_\mathrm{s}}\\
            Y_{i_p,1,N_\mathrm{s}}
        \end{bmatrix}G =
        \begin{bmatrix}
            \overline{\Psi}_{\hat{i},1,f}\\
            \widehat{Y}_{\hat{i}_p,1,f}
        \end{bmatrix},
    \end{align}
    $\mathrm{(ii)}$ and with $\widehat{Y}_{\hat{i}_p,1,f}$ as an asymptotically unbiased predictor %with respect to both past and future noise 
    with respect to future noise and conditioned on past data as $p\rightarrow\infty$.\todo{Compa-rative rates?\\also
    $N_\mathrm{s}\rightarrow\infty$}%
\end{thm}

\subsection{Auxiliary results}
The proof of Theorem~\ref{theorem:main_result} is deferred till after the treatment of several auxiliary results. The first of which is the following lemma.
\setcounter{thm}{0}
\begin{lem}\label{lem:relative_rates}\citep{Bauer2002,Chiuso2006}
    A necessary condition for consistent closed-loop subspace identification is that as $p\rightarrow\infty$, $N\rightarrow\infty$ such that
    \begin{align}\label{eq:relative_rates}
        \begin{split}
            p &\geq -\frac{d\log N}{2\log|\rho|}, \quad 1 < d < \infty,\\
            \lim_{p,N\rightarrow\infty} &\; \frac{p}{(\log N)^\alpha}=0, \quad \alpha < \infty,
        \end{split}
    \end{align}
\end{lem}
where $\rho$ is the eigenvalue of $\tilde{A}$ of maximum modulus.

\begin{lem}\label{lem:oblique_projections}\citep[Lemma~1]{VanOverschee1994,Katayama1999} Let $a$, $b$ and $c$ be random vectors with components in the Hilbert space $\mathscr{H}$, $\mathscr{A}=\text{span}\{a\}$, $\mathscr{B}=\text{span}\{b\}$ and $\mathscr{A}\cap\mathscr{B}=\{0\}$. Furthermore, let correlation matrices be exemplified by $\Sigma_{ca}=\mathbb{E}[ca^\top]$ and their conditional counterparts by $\Sigma_{ca|b}=\mathbb{E}[(c|b^\bot)(a|b^\bot)^\top]$, with $\bot$ denoting the orthogonal complement. Then the orthogonal projection of the row space of $c$ on the joint row space of $a$ and $b$ is the sum of the oblique projections $\Pi_{ca||b}a$ ($c$ onto $\mathscr{A}$ along $\mathscr{B}$) and $\Pi_{cb||a}b$ ($c$ onto $\mathscr{B}$ along $\mathscr{A}$) as
\begin{align}
    \begin{bmatrix}
        \Sigma_{ca} & \Sigma_{cb}
    \end{bmatrix}
    \begin{bmatrix}
        \Sigma_{aa} & \Sigma_{ab}\\ \Sigma_{ba} & \Sigma_{bb}
    \end{bmatrix}^\dagger
    \begin{bmatrix}
        a \\ b
    \end{bmatrix} = \Pi_{ca||b} a + \Pi_{cb||a} b,\label{eq:oblique_project}\\
    \text{s.t. }\quad\Pi_{ca||b}\Sigma_{aa|b} = \Sigma_{ca|b},\quad \Pi_{cb||a}\Sigma_{bb|a} = \Sigma_{cb|a}.\notag
\end{align}
The conditional correlation matrices may be expressed as demonstrated by $\Sigma_{ca|b}=\Sigma_{ca}-\Sigma_{cb}\Sigma_{bb}\inv\Sigma_{ba}$ if the involved inverse exists. In addition, $\Sigma_{aa|b}$ and $\Sigma_{bb|a}$ are invertible if $\Sigma_{aa}$ and $\Sigma_{bb}$ are invertible.
\end{lem}
\subsection{Proof of Theorem~\ref{theorem:main_result}}
% ---------------- Proof of (i) ----------------
\noindent\textbf{Proof of $(\mathrm{i})$:} In \eqref{eq:CL_DeePC_no_IVs} the output predictor is an optimization variable that is defined in terms of $G$, leaving its top matrix equations to specify $G$ as
\begin{align}\label{eq:G_sols}
    G = \Psi_{i,1,N_\mathrm{s}}^{\dagger,\mathrm{r}}\overline{\Psi}_{\hat{i},1,f} = \Psi_{i,1,N_\mathrm{s}}\inv\overline{\Psi}_{\hat{i},1,f},
\end{align}
in which, by definition, of the right inverse, $\Psi_{i,1,N_\mathrm{s}}^{\dagger,\mathrm{r}}=\Psi_{i,1,N_\mathrm{s}}^\top \left(\Psi_{i,1,N_\mathrm{s}}\Psi_{i,1,N_\mathrm{s}}^\top\right)\inv$. Since $\Psi_{i,1,N_\mathrm{s}}$ is square and invertible this reduces to $\Psi_{i,1,N_\mathrm{s}}\inv$. The solution to $G$ provided by \eqref{eq:G_sols} proves that $\exists G$ that satisfies \eqref{eq:CL_DeePC_no_IVs}. $\hfill\qed$
% 
% Decomposing $\Psi_{i,1,N}$ into its dependencies begets
% \begin{align}\label{eq:Phi_isN}
%     \mkern-8mu\Psi_{i,1,N}=\mkern-1mu\begin{bmatrix}
%         U_{i,p,N}\\
%         U_{i_p,1,N}\\
%         Y_{i,p,N}
%     \end{bmatrix}\mkern-1mu=\mkern-1mu
%     \underbrace{\begin{bmatrix}
%         0        & I_{pr} & 0      & 0\\
%         0        & 0      & I_{r} & 0\\
%         \Gamma_p & \mathcal{T}_p^\mathrm{u} & 0 & \mathcal{H}_p
%     \end{bmatrix}}\mkern-3mu\begin{bmatrix}
%         X_{i,1,N}\\
%         U_{i,p,N}\\
%         U_{i_p,1,N}\\
%         E_{i,p,N}
%     \end{bmatrix}\mkern-1mu,
% \end{align}
% in which the matrix on the right hand side is assumed to be full row rank, and the underbraced matrix is also full row rank. Since both of these matrices are full row rank, so is its product $\Psi_{i,1,N}$, meaning that there exists at least one $G$ that satisfies \eqref{eq:CL_DeePC_no_IVs}. 

% \noindent\textbf{Remark 1:} For the matrix on the right hand side of \eqref{eq:Phi_isN} to be guaranteed to be of full row rank a necessary condition is that $N\geq n+(p+1)r+pl$.

% \noindent\textbf{Remark 2:} The full row rank matrix $\Psi_{i,1,N}\in\mathbb{R}^{((p+1)r+pl)\times N}$ has more columns than rows, meaning that in fact there are an infinite number of possibilities for $G$ that satisfy \eqref{eq:CL_DeePC_no_IVs}. These possibilities are given by
% % \begin{align}\label{eq:G_sols}
% %     G = \Psi_{i,1,N}^\dagger\overline{\Psi}_{\hat{i},1,f} + \Pi_{\Psi_{i,1,N}}^\bot W,
% % \end{align}
% in which the dagger $\dagger$ denotes the right inverse ($\mathcal{Q}^\dagger=\mathcal{Q}^\top(\mathcal{Q}\mathcal{Q}^\top)\inv$ with $\mathcal{Q}$ as a real, full row rank matrix), $\Pi_{\Psi_{i,1,N}}^\bot=I_N-\Psi_{i,1,N}^\dagger\Psi_{i,1,N}$ is a projection matrix onto the orthogonal complement of the row space of $\Psi_{i,1,N}$, %see Overschee1996, pg. 19
% and $W\in\mathbb{R}^{N\times f}$ is a matrix of decision variables that parameterizes $G$.

% ---------------- Proof of (ii) ----------------
\noindent\textbf{Proof of $(\mathrm{ii})$:} Equation \eqref{eq:CL_DeePC_no_IVs} stipulates an output predictor as $\widehat{Y}_{\hat{i}_p,1,f}=Y_{i_p,1,N_\mathrm{s}}G$. %
Using \eqref{eq:DataEq1}, \eqref{eq:G_sols}, and considering $\Gamma_1=C$, $\mathcal{H}_1=I_l$, $L_1=\big[C\tKp{u} \;\;\! D \;\;\! C\tKp{y}\big]$ yields
\begin{align}
    \mkern-6mu\widehat{Y}_{\hat{i}_p,1,f} &= C \tilde{A}^p X_{i,1,N_\mathrm{s}}G + E_{i_p,1,N_\mathrm{s}}G + L_1 \Psi_{i,1,N_\mathrm{s}}G\notag\\
    \begin{split}
        &=\!\Big(\!\big(C \tilde{A}^p \underbrace{X_{i,1,N_\mathrm{s}}\Psi_{i,1,N_\mathrm{s}}^\top}_{=\hat{\Sigma}_{x\psi}} + \underbrace{E_{i_p,1,N_\mathrm{s}}\Psi_{i,1,N_\mathrm{s}}^\top}_{=\hat{\Sigma}_{e\psi}}\big)\times\\
        &\phantom{==}\big(\smash{\underbrace{\Psi_{i,1,N_\mathrm{s}}\Psi_{i,1,N_\mathrm{s}}^\top}_{=\hat{\Sigma}_{\psi\psi}}}\big)\inv+\begin{bmatrix}C\tKp{u} & D & C\tKp{y}\end{bmatrix}\Big)\overline{\Psi}_{\hat{i},1,f},
    \end{split}\label{eq:Yfhat_1}
\end{align}
in which the underbraced terms define the indicated sample correlation matrices. Partitioning the matrix $\Psi_{i,1,N_\mathrm{s}}=[U_{i,p+1,N_\mathrm{s}}^\top\;Y_{i,p,N_\mathrm{s}}^\top]^\top$ such that
\begin{alignat*}{3}
    \hat{\Sigma}_{\psi\psi}&=\begin{bmatrix}
        \hat{\Sigma}_{uu} & \hat{\Sigma}_{uy}\\
        \hat{\Sigma}_{yu} & \hat{\Sigma}_{yy}
    \end{bmatrix} &&= \begin{bmatrix} U_{i,p+1,N_\mathrm{s}} \\ Y_{i,p,N_\mathrm{s}} \end{bmatrix}\begin{bmatrix} U_{i,p+1,N_\mathrm{s}}^\top & Y_{i,p,N_\mathrm{s}}^\top \end{bmatrix},\\
    \hat{\Sigma}_{x\psi}&=\begin{bmatrix}
        \hat{\Sigma}_{xu} & \hat{\Sigma}_{xy}
    \end{bmatrix} &&= X_{i,1,N_\mathrm{s}}\begin{bmatrix} U_{i,p+1,N_\mathrm{s}}^\top & Y_{i,p,N_\mathrm{s}}^\top \end{bmatrix},\\
    \hat{\Sigma}_{e\psi}&=\begin{bmatrix}
        \hat{\Sigma}_{eu} & \hat{\Sigma}_{ey}
    \end{bmatrix} &&= E_{i_p,1,N_\mathrm{s}}\begin{bmatrix} U_{i,p+1,N_\mathrm{s}}^\top & Y_{i,p,N_\mathrm{s}}^\top \end{bmatrix},
\end{alignat*}
and applying Lemma~\ref{lem:oblique_projections}, \eqref{eq:Yfhat_1} is rewritten in terms of oblique sample projections as
\begin{align}\label{eq:Yfhat_2}
    \begin{split}
    \widehat{Y}_{\hat{i}_p,1,f} = \Big(C\tilde{A}^p\hat{\Pi}_{xy||u}+\hat{\Pi}_{ey||u}
    + C\tKp{y}\Big)&\overline{Y}_{\hat{i},p,f}\\
    +\Big(C\tilde{A}^p\hat{\Pi}_{xu||y}+\hat{\Pi}_{eu||y}+
    \begin{bmatrix}C\tKp{u} & D \end{bmatrix}\!\Big)&U_{\hat{i},p+1,f},
    \end{split}
\end{align}
with the sample-based oblique projection matrices
\begin{alignat*}{3}
    \hat{\Pi}_{xy||u}&=\hat{\Sigma}_{xy|u}\hat{\Sigma}_{yy|u}\inv, \qquad &\hat{\Pi}_{ey||u}&=\hat{\Sigma}_{ey|u}\hat{\Sigma}_{yy|u}\inv,\\
    \hat{\Pi}_{xu||y}&=\hat{\Sigma}_{xu|y}\hat{\Sigma}_{uu|y}\inv,        &\hat{\Pi}_{eu||y}&=\hat{\Sigma}_{eu|y}\hat{\Sigma}_{uu|y}\inv.
\end{alignat*}
Note that the invertibility of $\hat{\Sigma}_{yy|u}$ and $\hat{\Sigma}_{uu|y}$ is ensured according to Lemma~\ref{lem:oblique_projections} by the non-singularity of $\hat{\Sigma}_{yy}$ and $\hat{\Sigma}_{uu}$, which in turn derives from the full row rank of $\Psi_{i,1,N_\mathrm{s}}$.

Having derived the output predictor obtained from \eqref{eq:CL_DeePC_no_IVs}, now consider its error. Equations \eqref{eq:DataEq1} and \eqref{eq:Yfhat_2} determine this error as
%
%Consider the error of this prediction, which using \eqref{eq:DataEq1}, and considering $\Gamma_1=C$, $\mathcal{H}_1=I_l$ is written as
\begin{align}
    \begin{split}
        &\widehat{Y}_{\hat{i}_p,1,f}-Y_{\hat{i}_p,1,f} = C\tKp{y}\left(\overline{Y}_{\hat{i},p,f}-Y_{\hat{i},p,f}\right)\\
        &\;\;\;+C\tilde{A}^p\!\left(\hat{\Pi}_{xy||u}\overline{Y}_{\hat{i},p,f}+\hat{\Pi}_{xu||y} U_{\hat{i},p+1,f}-X_{\hat{i},1,f}\!\right)\\
        &\;\;\;+\hat{\Pi}_{eu||y}U_{\hat{i},p+1,f}+\hat{\Pi}_{ey||u}\overline{Y}_{\hat{i},p,f}-E_{\hat{i}_p,1,f}
    \end{split}\label{eq:Yf_error1}%\\
    % \begin{split}
    %     &\!\!\!\widehat{Y}_{\hat{i}_p,1,f}-Y_{\hat{i}_p,1,f} = C \tilde{A}^p (%\underbrace{
    %     X_{i,1,N_\mathrm{s}}G%}_{=\widehat{X}_{\hat{i},1,f}}
    %     -X_{\hat{i},1,f})-E_{\hat{i}_p,1,f}\\
    %     &\phantom{=}+ L_1(\smash{\underbrace{\Psi_{i,1,N_\mathrm{s}}G}_{\mathrlap{=\overline{\Psi}_{\hat{i},1,f}\;\because \text{ \eqref{eq:CL_DeePC_no_IVs}}}}}
    %     -\Psi_{\hat{i},1,f}) +%\underbrace{
    %     E_{i_p,1,N_\mathrm{s}}\underbrace{G}_{\mathclap{=\Psi_{i,1,N_\mathrm{s}}^{\dagger,\mathrm{r}}\overline{\Psi}_{\hat{i},1,f}\;\because\text{ \eqref{eq:G_sols}}}}%}_{=\widehat{E}_{\hat{i}_p,1,f}}
    % \end{split}\notag\\
    % \begin{split}
    %     &=C \tilde{A}^p (X_{i,1,N_\mathrm{s}}G-X_{\hat{i},1,f}) - E_{\hat{i}_p,1,f}\\
    %     &\phantom{=}+C\tKp{y}\left(\overline{Y}_{\hat{i},p,f}-Y_{\hat{i},p,f}\right)\\
    %     &\phantom{=}+\underbrace{E_{i_p,1,N_\mathrm{s}}\Psi_{i,1,N_\mathrm{s}}^\top}_{=\hat{\Sigma}_{e\psi}}(\underbrace{\Psi_{i,1,N_\mathrm{s}}\Psi_{i,1,N_\mathrm{s}}^\top}_{=\hat{\Sigma}_{\psi}})\inv \overline{\Psi}_{\hat{i},1,f},
    % \end{split}\label{eq:Yf_error1}
\end{align}
Applying the limit $p\rightarrow\infty$ asymptotically attenuates the error of the implicit initial state estimates on the second row since, by the definition of $K$ in \secref{sec:sys_model}, $\tilde{A}$ has all of its eigenvalues strictly inside the unit circle. Moreover, since $N_\mathrm{s}=(p+1)r+pl$, the limit $p\rightarrow\infty$ also ensures $N_\mathrm{s}\rightarrow\infty$. In this latter limit the sample correlation matrices approach their true correlation matrix with probability one because inputs and outputs are assumed to be quasi-stationary second-order ergodic stochastic processes. To this end, consider the sample correlation matrix $\hat{\Sigma}_{e\psi}$ that governs the oblique projection matrices $\hat{\Pi}_{eu||y}$ and $\hat{\Pi}_{ey||u}$ as indicated by \eqref{eq:oblique_project}.
\begin{align}\label{eq:E_Phi_correlation}
    \begin{split}
        &\hat{\Sigma}_{e\psi} = \\
        &\;\frac{1}{N_\mathrm{s}}\;\sum\limits_{k=i_p}^{\mathclap{i_p+N_\mathrm{s}-1}} e_k \begin{bmatrix}u_{k-p}^\top & \cdots & u_{k-1}^\top & u_k^\top & y_{k-p}^\top & \cdots & y_{k-1}^\top \end{bmatrix}.
    \end{split}
\end{align}
Due to the feedback of a (by assumption) strictly causal controller, inputs are correlated with preceding noise (${\mathbb{E}[e_k u_j^\top]\neq0,\; \forall j>k}$), but inputs are uncorrelated with concurrent and subsequent noise (${\mathbb{E}[e_k u_j^\top]=0,\; \forall j\leq k}$). Since the innovation noise is also uncorrelated with preceding outputs (${\mathbb{E}[e_k y_j^\top]=0,\; \forall j<k}$), there is no correlation between the relevant terms in \eqref{eq:E_Phi_correlation} and the expectation of the bottom row of \eqref{eq:Yf_error2} with respect to future noise is zero in the limit $p,N_\mathrm{s}\rightarrow\infty$.


\begin{alignat}{2}
\begin{split}\label{eq:Yf_error2}
    \!\!&\lim_{p\rightarrow\infty}\widehat{Y}_{\hat{i}_p,1,f}-Y_{\hat{i}_p,1,f} = \lim_{p\rightarrow\infty}\!\Big[C\tKp{y}\left(\overline{Y}_{\hat{i},p,f}-Y_{\hat{i},p,f}\right)\\
        &%+E_{i_p,1,N_\mathrm{s}} \Pi_{\Psi_{i,1,N_\mathrm{s}}}^\bot W
        \quad+\hat{\Pi}_{eu||y}U_{\hat{i},p+1,f}+\hat{\Pi}_{ey||u}\overline{Y}_{\hat{i},p,f}-E_{\hat{i}_p,1,f}\Big].
\end{split}
\end{alignat}%
Taking the expectation (denoted by $\mathbb{E}[\cdot]$) with respect to future noise as conditioned on past data (denoted by $\mathbb{E}^\mathrm{f}[\cdot]$) of \eqref{eq:Yf_error2} removes the dependence on future noise $E_{\hat{i}_p,1,f}$. Consider the correlation matrix
% For the predictor to be unbiased, the expectation (which we will denote with $\mathbb{E}[\cdot]$) of this error w.r.t. the noise must be zero. \todo{$\mathbb{E}[\cdot]$,\\$EW$,\\$\hat{\Sigma}\hat{\Sigma}\inv$?} Consider the underbraced correlation matrix
\begin{align}%\label{eq:E_Phi_correlation}
    \begin{split}
        &\hat{\Sigma}_{e\psi} = \\
        &\;\frac{1}{N_\mathrm{s}}\;\sum\limits_{k=i_p}^{\mathclap{i_p+N_\mathrm{s}-1}} e_k \begin{bmatrix}u_{k-p}^\top & \cdots & u_{k-1}^\top & u_k^\top & y_{k-p}^\top & \cdots & y_{k-1}^\top \end{bmatrix}.
    \end{split}
\end{align}
Since $N_\mathrm{s}=(p+1)r+pl$, the limit $p\rightarrow\infty$ also ensures $N_\mathrm{s}\rightarrow\infty$. By the assumed conditions of second-order ergodicity and quasi-stationarity, the correlation matrix of \eqref{eq:E_Phi_correlation} asymptotically approaches a well-defined true correlation matrix. Due to the feedback of a (by assumption) strictly causal controller, inputs are correlated with preceding noise (${\mathbb{E}[e_k u_j^\top]\neq0,\; \forall j>k}$), but inputs are uncorrelated with concurrent and subsequent noise (${\mathbb{E}[e_k u_j^\top]=0,\; \forall j\leq k}$). Since the innovation noise is also uncorrelated with preceding outputs (${\mathbb{E}[e_k y_j^\top]=0,\; \forall j<k}$), there is no correlation between the relevant terms in \eqref{eq:E_Phi_correlation} and the expectation of the bottom row of \eqref{eq:Yf_error2} with respect to future noise is zero in the limit $p,N_\mathrm{s}\rightarrow\infty$. %Since $E_{i_p,1,N}$ and $\Psi_{i,1,N}$ are uncorrelated, $\mathbb{E}[E_{i_p,1,N} \Pi_{\Psi_{i,1,N}}^\bot W]=\mathbb{E}[E_{i_p,1,N}W]=0$ such that the expectation of the second row of \eqref{eq:Yf_error2} is also zero.

Given the structure of $\overline{\Psi}_{\hat{i},1,f}$ and $\overline{Y}_{\hat{i},p,f}$ shown in Fig.~\ref{fig:CL-DeePC}, consider \eqref{eq:Yf_error2} column by column. As discussed, taking the expectation conditioned on past data leaves
\begin{align}\label{eq:Yf_error3}
\begin{split}
    &\mkern-3mu\lim_{p\rightarrow\infty} \mathbb{E}^\mathrm{f}\left[\hat{y}_{\hat{i}_p+k}-y_{\hat{i}_p+k}\right] = \\ &\;\;\;\lim_{p\rightarrow\infty}C\tKp{y}\mathbb{E}^\mathrm{f}\left[\datavec{\overline{y}}{\hat{i}+k,p}-\datavec{y}{\hat{i}+k,p}\right],\forall k\in[0,f-1].
\end{split}
\end{align}
By \eqref{eq:Yf_error3} in the limit $p\rightarrow\infty$ the prediction $\hat{y}_{\hat{i}_p+k}$ is unbiased if the $p$ preceding output estimates are unbiased. For $k=0$, this is the case because none of the relevant preceding output data is estimated ($\datavec{\overline{y}}{\hat{i},p}=\datavec{y}{\hat{i},p}$). No bias is thereby introduced on the right hand side for $k=1$ such that $\hat{y}_{\hat{i}_p+1}$ is also unbiased. Repetition of this process until $k=f-1$ demonstrates that, in the limit $p\rightarrow\infty$, $\widehat{Y}_{\hat{i}_p,1,f}$ is indeed an asymptotically unbiased predictor with respect to future noise and conditioned on past data. This concludes the proof of $\mathrm{(ii)}$. $\hfill\qed$

\subsection{Systematic noise mitigation using \ac{IVs}}
The previous section demonstrated that the predictor that is obtained from \eqref{eq:CL_DeePC_no_IVs} is asymptotically unbiased. This section demonstrates the use of \ac{IVs} as a means to decrease the variance of this predictor.

Since the relevant innovation and input-output samples in the correlation matrix of \eqref{eq:E_Phi_correlation} are uncorrelated, the underbraced term in \eqref{eq:Yf_error2} vanishes asymptotically as $N_\mathrm{s}\rightarrow\infty$. Since $N_\mathrm{s}=(p+1)r+pl$, the rate at which the underbraced term vanishes in \eqref{eq:Yf_error2} is determined by the rate at which $p\rightarrow\infty$. For a finite $p$ it would be favorable to use a number of columns $N>N_\mathrm{s}$ such that the error induced by the correlation in \eqref{eq:E_Phi_correlation} is smaller. To this end, the following theorem makes use of an (extended) \ac{IV} $\mathcal{Z}$.
% 
% are increasing the variance of the implicitly estimated noise in \todo{left off here}$\widehat{E}_{\hat{i}_p,1,f}$. The idea is to redefine $G$ such that its columns are orthogonal to the noise. The following theorem demonstrates the use of an \ac{IV} $\mathcal{Z}$ for this purpose based on the same assumptions as Theorem~\ref{theorem:main_result}.

\begin{thm}\label{theorem:main_result_IVs}
    % Consider the minimal discrete non-deterministic \ac{LTI} system given by~\eqref{eqn:SS_innovation} to generate input-output data in closed-loop with a strictly causal controller. Define data matrices $\Psi_{i,1,N}$ and $\overline{\Psi}_{\hat{i},1,f}$ as in \eqref{eq:Phi_def}. %
    % 
    % If the joint input and noise sequences are sufficiently persistently exciting such that $\left[X_{i,1,N}^\top \; U_{i,p,N}^\top \; U_{i_p,1,N}^\top \; E_{i,p,N}^\top\right]^\top$ is full row rank, and with the choice of \ac{IV} given by $\mathcal{Z}=\Psi_{i,1,N}$ then\\
    Consider the minimal discrete non-deterministic \ac{LTI} system given by~\eqref{eqn:SS_innovation} to generate input-output data in closed-loop by means of a causal controller without direct feedthrough, data matrices $\overline{\Psi}_{\hat{i},1,f}$ and full row rank $\Psi_{i,1,N}$ as in \eqref{eq:Phi_def}, and with an \ac{IV} given by $\mathcal{Z}=\Psi_{i,1,N}$ then\\
    $\mathrm{(i)}$ $\exists G^\mathrm{IV}\in\mathbb{R}^{((p+s)r+pl)\times f}$ such that
    \begin{align}\label{eq:Theorem2}
        \begin{bmatrix}
            \Psi_{i,1,N}\\Y_{i_p,1,N}
        \end{bmatrix}\mathcal{Z}^\top G^\mathrm{IV} =
        \begin{bmatrix}
            \overline{\Psi}_{\hat{i},1,f}\\\widehat{Y}_{\hat{i}_p,1,f}^\mathrm{IV}
        \end{bmatrix},
    \end{align}
    $\mathrm{(ii)}$ and with $\widehat{Y}_{\hat{i}_p,1,f}^\mathrm{IV}$ as an asymptotically unbiased predictor with respect to future noise and conditioned on past data as $p\rightarrow\infty$, $N\rightarrow\infty$.\\
    $\mathrm{(iii)}$ that has a variance that is smaller than or equal to the variance of $\widehat{Y}_{\hat{i}_p,1,f}$.
\end{thm}
\textbf{Proof of $\mathrm{(i)}:$} Similarly as before, $G^\mathrm{IV}$ is determined by the top matrix equations of \eqref{eq:Theorem2}, which are given by $\Psi_{i,1,N}\mathcal{Z}^\top G^\mathrm{IV}=\overline{\Psi}_{\hat{i},1,f}$. By the same reasoning as in the proof of Theorem~\ref{theorem:main_result}$\mathrm{(i)}$ the matrix $\Psi_{i,1,N}$ is full row rank. Hence, $\Psi_{i,1,N}\mathcal{Z}^\top=\Psi_{i,1,N}\Psi_{i,1,N}^\top$ is square and invertible such that $G^\mathrm{IV} = (\Psi_{i,1,N}\Psi_{i,1,N}^\top)\inv \overline{\Psi}_{\hat{i},1,f}$. $\hfill \qed$\\
\textbf{Remark 3:} Note that $G$ in Theorem~\ref{theorem:main_result} has been replaced with $\mathcal{Z}^\top G^\mathrm{IV}$ in Theorem~\ref{theorem:main_result_IVs}, which is
\begin{align*}
    \mathcal{Z}^\top G^\mathrm{IV} = \Psi_{i,1,N}^\dagger \overline{\Psi}_{\hat{i},1,f}.
\end{align*}
This is the same as $G$ as defined in \eqref{eq:G_sols}, but with $\Pi_{\Psi_{i,1,N}}^\bot W=0$. Essentially $G$ is restricted to lie in the row space of $\Psi_{i,1,N}$.\\
\textbf{Proof of $\mathrm{(ii)}$:} Based on Remark 3 the asymptotic unbiasedness of $\widehat{Y}_{\hat{i},1,f}^\mathrm{IV}$ as $p\rightarrow\infty$ follows directly from the proof of Theorem~\ref{theorem:main_result}$\mathrm{(ii)}$ with $\Pi_{\Psi_{i,1,N}}^\bot W=0$. $\hfill \qed$\\
\textbf{Proof of $\mathrm{(iii)}$:} This proof requires it to be shown that
\begin{align}\label{eq:CovarianceDecrease}
\begin{split}
     \lim_{p,N\rightarrow\infty} \mathbb{E}&\left[(\datavec{\hat{y}}{\hat{i}_p,f}-\datavec{y}{\hat{i}_p,f})(\datavec{\hat{y}}{\hat{i}_p,f}-\datavec{y}{\hat{i}_p,f})^\top\right]\\
   -\mathbb{E} &\left[(\datavec{\hat{y}}{\hat{i}_p,f}^\mathrm{IV}-\datavec{y}{\hat{i}_p,f})(\datavec{\hat{y}}{\hat{i}_p,f}^\mathrm{IV}-\datavec{y}{\hat{i}_p,f})^\top\right] \succeq 0,   
\end{split}
\end{align}
in which $\datavec{\hat{y}}{\hat{i}_p,f}=\text{vec}(\widehat{Y}_{\hat{i}_p,1,f})$, and $\datavec{\hat{y}}{\hat{i}_p,f}^\mathrm{IV}=\text{vec}(\widehat{Y}_{\hat{i}_p,1,f}^\mathrm{IV})$. Applying the limit $N\rightarrow\infty$ to \eqref{eq:Yf_error2} makes the sample correlation in \eqref{eq:E_Phi_correlation} converge to zero, thereby leaving after vectorization
\begin{align*}
\begin{split}
    \lim_{p,N\rightarrow\infty} &\datavec{\hat{y}}{\hat{i}_p,f}-\datavec{y}{\hat{i}_p,f}%\widehat{Y}_{\hat{i}_p,1,f}-Y_{\hat{i}_p,1,f}
     = \lim_{p,N\rightarrow\infty}\Big[\\
     &\big((\Pi_{\Psi_{i,1,N}}^\bot W)^\top \otimes I_l\big)\datavec{e}{i_p,N}-\datavec{e}{\hat{i}_p,f}\\
     +&\underbrace{\big(I_f \otimes C\tKp{y}\big)\text{vec}\left(\overline{Y}_{\hat{i},p,f}-Y_{\hat{i},p,f}\right)}_{=(I_{fl}-\mathcal{\widetilde{H}}_{f})(\datavec{\hat{y}}{\hat{i}_p,f}-\datavec{y}{\hat{i}_p,f})}\Big].
\end{split}
\end{align*}
The underbraced term can be replaced as indicated since $\lim_{p\rightarrow\infty}\tilde{A}^p=0$ such that
\begin{align}\label{eq:Yf_error4}
\begin{split}
    \lim_{p,N\rightarrow\infty} &\datavec{\hat{y}}{\hat{i}_p,f}-\datavec{y}{\hat{i}_p,f}%\widehat{Y}_{\hat{i}_p,1,f}-Y_{\hat{i}_p,1,f}
     = \lim_{p,N\rightarrow\infty}\Big[\\
     &\mathcal{H}_f\big((\Pi_{\Psi_{i,1,N}}^\bot W)^\top \otimes I_l\big)\datavec{e}{i_p,N}-\mathcal{H}_f\datavec{e}{\hat{i}_p,f}\Big],
\end{split}
\end{align}
where $\mathcal{H}_f=\mathcal{\widetilde{H}}_f\inv$~\citep[Lemma~1]{Houtzager2012}. Similarly then, based on Remark~3 and \eqref{eq:Yf_error4}
\begin{align}\label{eq:Yf_error5}
    \lim_{p,N\rightarrow\infty} \datavec{\hat{y}}{\hat{i}_p,f}^\mathrm{IV}-\datavec{y}{\hat{i}_p,f}%\widehat{Y}_{\hat{i}_p,1,f}-Y_{\hat{i}_p,1,f}
     = \lim_{p,N\rightarrow\infty}-\mathcal{H}_f\datavec{e}{\hat{i}_p,f}.
\end{align}
Applying the found errors from \eqref{eq:Yf_error4} and \eqref{eq:Yf_error5} to \eqref{eq:CovarianceDecrease} obtains
\begin{align}\label{eq:Cov}
\begin{split}
        &\lim_{p,N\rightarrow\infty} \mathbb{E}\left[(\datavec{\hat{y}}{\hat{i}_p,f}-\datavec{y}{\hat{i}_p,f})(\datavec{\hat{y}}{\hat{i}_p,f}-\datavec{y}{\hat{i}_p,f})^\top\right]\\
        &\qquad\mkern1mu-\mathbb{E}\left[(\datavec{\hat{y}}{\hat{i}_p,f}^\mathrm{IV}-\datavec{y}{\hat{i}_p,f})(\datavec{\hat{y}}{\hat{i}_p,f}^\mathrm{IV}-\datavec{y}{\hat{i}_p,f})^\top\right]
\end{split}\notag\\
\begin{split}
        &=\lim_{p,N\rightarrow\infty}\Big\{\mathcal{H}_f\psi\mathbb{E}[\datavec{e}{i_p,N}\datavec{e}{i_p,N}^\top]\psi^\top\mathcal{H}_f^\top\\
        &\qquad\mkern1mu-\mathcal{H}_f\psi\mathbb{E}[\datavec{e}{i_p,N}\datavec{e}{\hat{i}_p,f}^\top]-\mathbb{E}[\datavec{e}{\hat{i}_p,f}\datavec{e}{i_p,N}^\top]\psi^\top\mathcal{H}_f^\top\Big\}
\end{split}\notag\\
&=\lim_{p,N\rightarrow\infty}\Big\{\mathcal{H}_f\psi(I_N\otimes R_\mathrm{e})\psi^\top\mathcal{H}_f^\top\Big\}\notag\\
&=\lim_{p,N\rightarrow\infty}\Big\{\mathcal{H}_f\psi\big(I_N\otimes R_\mathrm{e}^{1/2}\big)\big(I_N\otimes R_\mathrm{e}^{\top/2}\big)\psi^\top\mathcal{H}_f^\top\Big\}\notag\succeq 0,
\end{align}
in which use is made of $\psi=(\Pi_{\Psi_{i,1,N}}^\bot W)^\top \otimes I_l$ for brevity, the Cholesky factorization $R_\mathrm{e}=R_\mathrm{e}^{1/2}R_\mathrm{e}^{\top/2}$ (since ${R_\mathrm{e}\succ0}$), and the fact that past and future innovations ($\datavec{e}{i_p,N}$ and $\datavec{e}{\hat{i}_p,f}$ respectively) are uncorrelated. $\hfill  \qed$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
