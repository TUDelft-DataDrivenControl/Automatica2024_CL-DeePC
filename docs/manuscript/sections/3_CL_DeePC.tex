\section{Closed-loop Data-enabled Predictive Control}
\noindent This section presents the \ac{CL-DeePC} algorithm, being the main result of this article and providing contribution~(\ref{contribution:develop_CL_DeePC}). An intuitive explanation is first offered before a proof of the underlying main result is provided.

A solution to the identification bias that arises in closed-loop due to correlation between inputs and noise %(a demonstration thereof is deferred to Section~\ref{sec:CL_ID_issue})
is the synthesis of a one step-ahead predictor~\citep{Ljung1996}. For receding horizon optimal control settings with a typically more useful, larger prediction horizon ($f>1$), the previously described one step-ahead predictor can be applied sequentially.

To explain how sequential application of a one step-ahead predictor is used in \ac{CL-DeePC} consider Fig.~\ref{fig:CL-DeePC} and Fig.~\ref{fig:regular-DeePC}, which respectively illustrate \ac{CL-DeePC} and \ac{DeePC} formulations without \ac{IVs}. Colors differentiate between known data (black) and unknown optimization variables (red). Each dot represents either an input ${u_k\in\mathbb{R}^r}$, output ${y_k\in\mathbb{R}^l}$, or element of the matrix $G$ (in \ac{CL-DeePC}) or vector $g$ (in \ac{DeePC}). A one-step ahead predictor can be obtained from \ac{DeePC} with $f=1$ in \eqref{eq:regular_DeePC_no_IVs}. In \ac{CL-DeePC} the successive columns of $G$ ($g_1$ to $g_f$ in \eqref{eq:CL_DeePC_no_IVs}) and their corresponding columns on the right-hand side correspond to sequential applications of \ac{DeePC} with $f=1$ to the same matrix of known sufficiently persistently exciting past input-output data on the left-hand side. On the right hand side, each successive column in \ac{CL-DeePC} (from left to right) contains inputs and outputs from a time window that is shifted a single sample further into the future. This results in a composition of block-Hankel matrices with the dashed block-anti-diagonals containing the same input or output (prediction). As the window of data represented by consecutive columns shifts further into the future the proportion of known past data in black naturally decreases.
%
%Fig.~\ref{fig:CL-DeePC} and \eqref{eq:CL_DeePC_no_IVs} illustrate how this idea is employed in \ac{CL-DeePC}. A one step-ahead predictor can be obtained from \ac{DeePC} (see Fig.~\ref{fig:regular-DeePC} and \eqref{eq:regular_DeePC_no_IVs}) with $f=1$. In \ac{CL-DeePC} the successive columns of $G$ (from left to right) and their corresponding columns on the right-hand side correspond to sequential applications of \ac{DeePC} with $f=1$ to the same matrix of sufficiently persistently exciting past input-output data on the left-hands side. Time-shifted windows of input-output data on the right-hand side encode information on successive initial states.
\subsection{Main result}
\noindent To ensure that \ac{CL-DeePC} employs a consistent output predictor the following main result of this article concerns a modification of \eqref{eq:CL_DeePC_no_IVs} that makes use of \ac{IVs}.
\begin{figure}[b!]
\centering
\input{tikzpictures/CL-DeePC}
\caption{Visualization of known (black) and unknown (red) variables in \ac{CL-DeePC} without \ac{IVs}. Each dot represents an input $u_k\in\mathbb{R}^r$, output $y_k\in\mathbb{R}^l$, or element of the matrix $G$. \ac{CL-DeePC} involves $f$ sequential applications of a step-ahead predictor obtained from regular \ac{DeePC} with $f=1$ (see also Fig.~\ref{fig:regular-DeePC}), resulting in the dashed block-anti diagonals with the same $u_k$ or $y_k$ on the right hand side.}
\label{fig:CL-DeePC}
\end{figure}
\begin{figure}[b!]
\centering
\input{tikzpictures/regular_DeePC}
\caption{Visualization of known (black) and unknown (red) variables in \ac{DeePC} without \ac{IVs}. Each dot represents an input $u_k\in\mathbb{R}^r$, output $y_k\in\mathbb{R}^l$, or element of the vector $g$. A multi-step ahead predictor of prediction length $f$ is formed directly by taking a linear combination of past input and output data.\\\vspace{0.75mm}}
\label{fig:regular-DeePC}
\end{figure}
%
% ----------------------------------------------------------- Main result ---------------------------------------------------------------------------------------------
\setcounter{thm}{0}
\begin{thm}\label{theorem:main_result_IVs}
    % Consider the minimal discrete non-deterministic \ac{LTI} system given by~\eqref{eqn:SS_innovation} to generate input-output data in closed-loop with a strictly causal controller. Define data matrices $\Psi_{i,1,N}$ and $\overline{\Psi}_{\hat{i},1,f}$ as in \eqref{eq:Psi_def}. %
    % 
    % If the joint input and noise sequences are sufficiently persistently exciting such that $\left[X_{i,1,N}^\top \; U_{i,p,N}^\top \; U_{i_p,1,N}^\top \; E_{i,p,N}^\top\right]^\top$ is full row rank, and with the choice of \ac{IV} given by $\mathcal{Z}=\Psi_{i,1,N}$ then\\
    Consider the minimal discrete non-deterministic \ac{LTI} system given by~\eqref{eqn:SS_innovation} to generate input-output data in closed-loop by means of a causal controller without direct feedthrough, data matrices $\overline{\Psi}_{\hat{i},1,f}$ and full row rank $\Psi_{i,1,N}$ as in \eqref{eq:Psi_def}, and with an \ac{IV} given by $\mathcal{Z}=\Psi_{i,1,N}$, then\\
    $\mathrm{(i)}$ $\exists G^\mathrm{IV}\in\mathbb{R}^{((p+1)r+pl)\times f}$ such that
    \begin{align}\label{eq:TheoremIV}
        \begin{bmatrix}
            \Psi_{i,1,N}\\Y_{i_p,1,N}
        \end{bmatrix}\mathcal{Z}^\top G^\mathrm{IV} =
        \begin{bmatrix}
            \overline{\Psi}_{\hat{i},1,f}\\\widehat{Y}_{\hat{i}_p,1,f}^\mathrm{IV}
        \end{bmatrix},
    \end{align}
    $\mathrm{(ii)}$ which specifies $\widehat{Y}_{\hat{i}_p,1,f}^\mathrm{IV}$ as an asymptotically unbiased predictor as $p\rightarrow\infty$ and $N\rightarrow\infty$ in accordance with \eqref{eq:relative_rates}. %with respect to future noise and conditioned on past data as $p\rightarrow\infty$, $N\rightarrow\infty$.\\
    %$\mathrm{(iii)}$ that has a variance that is smaller than or equal to the variance of $\widehat{Y}_{\hat{i}_p,1,f}$.
\end{thm}
%
% \begin{thm}\label{theorem:main_result}
%     Consider the minimal discrete non-deterministic \ac{LTI} system given by~\eqref{eqn:SS_innovation} to generate input-output data in closed-loop by means of a causal controller without direct feedthrough, and data matrices $\overline{\Psi}_{\hat{i},1,f}$ and $\Psi_{i,1,N_\mathrm{s}}$ as in \eqref{eq:Phi_def}. If $N_\mathrm{s}=(p+1)r+pl$ such that $\Psi_{i,1,N_\mathrm{s}}$ is square, and furthermore, if $\Psi_{i,1,N_\mathrm{s}}$ is invertible, %
%     %
%     % If the joint input and noise sequences are sufficiently persistently exciting such that $\left[X_{i,1,N}^\top \; U_{i,p,N}^\top \; U_{i_p,1,N}^\top \; E_{i,p,N}^\top\right]^\top$ is full row rank %
%     % ------------------------------------------------------
%     %% ----------------- old version below -----------------
%     %If the input sequence $\{u_k\}_{k=i}^{i+\bar{N}-1}$ of length $\bar{N}=p+s+N-1$ %, with $N\geq(p+s+n)(r+l)+n$ and $p\geq\ell$\todo{don't forget},
%     % is persistently exciting of order $p+s+n$, and has sample correlations such that%
%     % \begin{alignat}{2}%see also https://www.cis.upenn.edu/~jean/schur-comp.pdf
%     % % \widehat{\Sigma}_{u,u} &> 0,\label{eq:PE_corU}\\
%     % &\widehat{\Sigma}_{\mathrm{ee}} - \widehat{\Sigma}_{\mathrm{ue}^\top} \widehat{\Sigma}_{\mathrm{uu}}\inv \widehat{\Sigma}_{\mathrm{ue}}\succ0,\span\span\label{eq:PE_corUE2}\\
%     % &&\text{with}\quad\widehat{\Sigma}_{\mathrm{ee}}&=E_{i,p+s+n,N-n}E_{i,p+s+n,N-n}^\top,\notag\\
%     % &&\widehat{\Sigma}_{\mathrm{ue}}&=U_{i,p+s+n,N-n}E_{i,p+s+n,N-n}^\top,\notag\\
%     % &&\widehat{\Sigma}_{\mathrm{uu}}&=U_{i,p+s+n,N-n}U_{i,p+s+n,N-n}^\top,\notag
%     % \end{alignat}
%     % ------------------------------------------------------
%     then \\
%     $\mathrm{(i)}$ $\exists G\in\mathbb{R}^{N\times f}$ such that
%     \begin{align}\tag{\ref{eq:CL_DeePC_no_IVs}}%\label{eq:Theorem1}
%         \begin{bmatrix}
%             \Psi_{i,1,N_\mathrm{s}}\\
%             Y_{i_p,1,N_\mathrm{s}}
%         \end{bmatrix}G =
%         \begin{bmatrix}
%             \overline{\Psi}_{\hat{i},1,f}\\
%             \widehat{Y}_{\hat{i}_p,1,f}
%         \end{bmatrix},
%     \end{align}
%     $\mathrm{(ii)}$ and with $\widehat{Y}_{\hat{i}_p,1,f}$ as an asymptotically unbiased predictor %with respect to both past and future noise 
%     with respect to future noise and conditioned on past data as $p\rightarrow\infty$.\todo{Compa-rative rates?\\also
%     $N_\mathrm{s}\rightarrow\infty$}%
% \end{thm}

% ----------------------------------------------------------- Auxiliary results ---------------------------------------------------------------------------------------------
\subsubsection{Auxiliary result}
\noindent Proof of Theorem~\ref{theorem:main_result_IVs} is deferred till after the following important auxiliary result.
\setcounter{thm}{0}
\begin{lem}\label{lem:relative_rates}\citep{Chiuso2006}
    For scalars $d,\alpha\in\mathbb{R}$, and ${\rho=\max|\emph{eig}(\tilde{A})|<1}$, if the past data window length ${p\rightarrow\infty}$, and the number of columns used to construct block-Hankel data matrices $N\rightarrow\infty$ such that%A necessary condition for consistent closed-loop subspace identification is that as $p\rightarrow\infty$, $N\rightarrow\infty$ such that
    \begin{align}\label{eq:relative_rates}
        \begin{split}
            p &\geq -\frac{d\log N}{2\log|\rho|} \quad 1 < d < \infty,\\
            p&=o((\log N)^\alpha) \quad \alpha < \infty,%\lim_{p,N\rightarrow\infty} &\; \frac{p}{(\log N)^\alpha}=0, \quad \alpha < \infty,
        \end{split}
    \end{align}
    then errors due to mis-specification of the initial condition are $o_P(1/\sqrt{N})$.
\end{lem}
% For the considered system that was introduced in Section~\secref{sec:sys_model} we have $|\rho|<1$. The conditions formulated by \eqref{eq:relative_rates} are necessary to ensure consistency of the closed-loop estimator because they ensure that errors induced by implicit mis-specification of the initial state are negligible and that sample correlation matrices asymptotically approach well-defined true correlation counterparts \citep{Chiuso2007,Bauer2002}.

% ----------------------------------------------------------- Proof of main result ---------------------------------------------------------------------------------------------
\subsubsection{Proof of Theorem~\ref{theorem:main_result_IVs}}\label{sec:proof_IVs}
% ---------------- Proof of (i) ----------------
\noindent\textbf{Proof of $(\mathrm{i})$:} In \eqref{eq:TheoremIV} $G^\mathrm{IV}$ is specified by the top row as
\begin{align}\label{eq:G_sols}
    G^\mathrm{IV} = \left(\Psi_{i,1,N}\mathcal{Z}^\top\right)\inv\overline{\Psi}_{\hat{i},1,f},
\end{align}
where the inverse exists because by choice, $\mathcal{Z}=\Psi_{i,1,N}$, which is full row rank by assumption. Note that \eqref{eq:G_sols} is consistent with the bottom row of \eqref{eq:TheoremIV} because $\widehat{Y}_{\hat{i}_p,1,f}^\mathrm{IV}$ is unknown a priori. $\hfill\qed$
% 
% Decomposing $\Psi_{i,1,N}$ into its dependencies begets
% \begin{align}\label{eq:Phi_isN}
%     \mkern-8mu\Psi_{i,1,N}=\mkern-1mu\begin{bmatrix}
%         U_{i,p,N}\\
%         U_{i_p,1,N}\\
%         Y_{i,p,N}
%     \end{bmatrix}\mkern-1mu=\mkern-1mu
%     \underbrace{\begin{bmatrix}
%         0        & I_{pr} & 0      & 0\\
%         0        & 0      & I_{r} & 0\\
%         \Gamma_p & \mathcal{T}_p^\mathrm{u} & 0 & \mathcal{H}_p
%     \end{bmatrix}}\mkern-3mu\begin{bmatrix}
%         X_{i,1,N}\\
%         U_{i,p,N}\\
%         U_{i_p,1,N}\\
%         E_{i,p,N}
%     \end{bmatrix}\mkern-1mu,
% \end{align}
% in which the matrix on the right hand side is assumed to be full row rank, and the underbraced matrix is also full row rank. Since both of these matrices are full row rank, so is its product $\Psi_{i,1,N}$, meaning that there exists at least one $G$ that satisfies \eqref{eq:CL_DeePC_no_IVs}. 

% \noindent\textbf{Remark 1:} For the matrix on the right hand side of \eqref{eq:Phi_isN} to be guaranteed to be of full row rank a necessary condition is that $N\geq n+(p+1)r+pl$.

% \noindent\textbf{Remark 2:} The full row rank matrix $\Psi_{i,1,N}\in\mathbb{R}^{((p+1)r+pl)\times N}$ has more columns than rows, meaning that in fact there are an infinite number of possibilities for $G$ that satisfy \eqref{eq:CL_DeePC_no_IVs}. These possibilities are given by
% % \begin{align}\label{eq:G_sols}
% %     G = \Psi_{i,1,N}^\dagger\overline{\Psi}_{\hat{i},1,f} + \Pi_{\Psi_{i,1,N}}^\bot W,
% % \end{align}
% in which the dagger $\dagger$ denotes the right inverse ($\mathcal{Q}^\dagger=\mathcal{Q}^\top(\mathcal{Q}\mathcal{Q}^\top)\inv$ with $\mathcal{Q}$ as a real, full row rank matrix), $\Pi_{\Psi_{i,1,N}}^\bot=I_N-\Psi_{i,1,N}^\dagger\Psi_{i,1,N}$ is a projection matrix onto the orthogonal complement of the row space of $\Psi_{i,1,N}$, %see Overschee1996, pg. 19
% and $W\in\mathbb{R}^{N\times f}$ is a matrix of decision variables that parameterizes $G$.

% ---------------- Proof of (ii) ----------------
\noindent\textbf{Proof of $(\mathrm{ii})$:} Applying \eqref{eq:G_sols} to \eqref{eq:TheoremIV} obtains
\begin{align}\label{eq:Yfhat_1}
    \widehat{Y}_{\hat{i}_p,1,f}^\mathrm{IV} = Y_{i_p,1,f}\mathcal{Z}^\top\left(\Psi_{i,1,N}\mathcal{Z}^\top\right)\inv\overline{\Psi}_{\hat{i},1,f}.
\end{align}
Substitution of $Y_{i_p,1,f}$ from \eqref{eq:DataEq2} results in
% Equation \eqref{eq:TheoremIV} stipulates an output predictor as $\widehat{Y}_{\hat{i}_p,1,f}^\mathrm{IV}=Y_{i_p,1,N}\mathcal{Z}^\top G^\mathrm{IV}$. %
% Using \eqref{eq:DataEq1}, \eqref{eq:G_sols}, and considering $\Gamma_1=C$, $\mathcal{H}_1=I_l$, $L_1=\big[C\tKp{u} \;\;\! D \;\;\! C\tKp{y}\big]$ yields
\begin{align}\label{eq:Yfhat_2}
    \begin{split}
        \mkern-6mu\widehat{Y}_{\hat{i}_p,1,f}^\mathrm{IV} = &\bigg(\widetilde{L}_1  + \underbrace{E_{i_p,1,N}\mathcal{Z}^\top}_{=\hat{\Sigma}_{ez}}\big(\underbrace{\Psi_{i,1,N}\mathcal{Z}^\top}_{=\hat{\Sigma}_{\psi z}}\big)\inv\bigg) \overline{\Psi}_{\hat{i},1,f}\\%G^\mathrm{IV}\\
        &+ C \tilde{A}^p X_{i,1,N}\mathcal{Z}^\top G^\mathrm{IV}.
    \end{split}
\end{align}
For $p,N\rightarrow\infty$, in accordance with \eqref{eq:relative_rates}, the initial state contribution may be neglected using Lemma~\ref{lem:relative_rates} since for the considered system $\rho<1$. Moreover, these conditions ensure that the underbraced sample correlations ($\hat{\Sigma}$), approach their well-defined true correlation counterparts~($\Sigma$)~\citep{Chiuso2007,Bauer2002}. With the choice of \ac{IV} of $\mathcal{Z}=\Psi_{i,1,N}$, for the correlation matrix $\hat{\Sigma}_{ez}=\hat{\Sigma}_{e\psi}$ this entails that
\begin{align}\label{eq:E_Phi_correlation}
    \!\!\!\!\begin{split}
        \lim_{\underset{\eqref{eq:relative_rates}}{p,N\rightarrow\infty}}\!\!\hat{\Sigma}_{e\psi} &= \!\lim_{\underset{\eqref{eq:relative_rates}}{p,N\rightarrow\infty}}\frac{1}{N}\;\sum\limits_{k=i_p}^{\mathclap{i_p+N-1}} e_k\!\begin{bmatrix}\datavec{u}{k-p,p+1}^\top & \datavec{y}{k-p,p}^\top \end{bmatrix}\\
        =\Sigma_{e\psi} &=\mathbb{E}\left[e_k\!\begin{bmatrix}\datavec{u}{k-p,p+1}^\top & \datavec{y}{k-p,p}^\top \end{bmatrix}\right]=0.
    \end{split}
\end{align}
The last equality in \eqref{eq:E_Phi_correlation} holds due to the feedback of a (by assumption) strictly causal controller, for which inputs are correlated with preceding noise (${\mathbb{E}[e_k u_j^\top]\neq0,\; \forall j>k}$), but inputs are uncorrelated with concurrent and subsequent noise (${\mathbb{E}[e_k u_j^\top]=0,\; \forall j\leq k}$). Moreover, the innovation noise is then also uncorrelated with preceding outputs (${\mathbb{E}[e_k y_j^\top]=0,\; \forall j<k}$).

By the foregoing analysis, with $\Sigma_{e\psi}=0$ and a vanished initial state contribution, the bias of the predicted output from \eqref{eq:Yfhat_2} w.r.t. the true output from \eqref{eq:DataEq2} is
\begin{align}\label{eq:asym_bias_IV}
    \lim_{\underset{\eqref{eq:relative_rates}}{p,N\rightarrow\infty}}\!\!\mathbb{E}\left[\widehat{Y}_{\hat{i}_p,1,f}^\mathrm{IV}-Y_{\hat{i}_p,1,f}\right]\!=\!\!\lim_{\underset{\eqref{eq:relative_rates}}{p,N\rightarrow\infty}}\!\!\widetilde{L}_1\mathbb{E}\left[\overline{\Psi}_{\hat{i},1,f}-\Psi_{\hat{i},1,f}\right]\!.
\end{align}
The proof of asymptotic unbiasedness is sequential. Note from Fig~\ref{fig:CL-DeePC} that the leftmost columns of $\overline{\Psi}_{\hat{i},1,f}$ and $\Psi_{\hat{i},1,f}$ are equal, so the asymptotic bias of the estimate of this column is zero. Moreover, in combination with \eqref{eq:asym_bias_IV}, Fig~\ref{fig:CL-DeePC} shows that the asymptotic bias of subsequent columns to the right is a linear combination of the asymptotic bias of preceding columns to the left. Hence, since the leftmost column is asymptotically unbiased, so are the others in \eqref{eq:asym_bias_IV}. $\hfill\qed$
\setcounter{thm}{0}
\begin{rem}
    Different choices of $\mathcal{Z}$ for which $\Sigma_{\psi z}$ is invertible and $\Sigma_{ez}=0$ are equally valid. The choice of $s=1$ in \eqref{eq:Psi_def} ensures this latter condition with $\mathcal{Z}=\Psi_{i,1,N}$.
\end{rem}
\begin{rem}
    The \ac{IV} used in this proof serves to reduce the number of decision variables as well as to handle noise in such a way as to guarantee the use of a consistent estimator. Without an \ac{IV}, as with \eqref{eq:CL_DeePC_no_IVs}, increasing $N$ both increases the number of decision variables and permits the optimal control problem to select noise in such a way that infeasible trajectories are predicted. As an alternative one may attempt to make the data matrix $\Psi_{i,1,N}$ square and invertible, but then \eqref{eq:relative_rates} is violated in the limit $p,N\rightarrow\infty$ such that the resulting predictor is asymptotically biased.
\end{rem}
% \begin{rem}
%     To choose a practical, finite $p$ one may consider the instructive remarks in, e.g,~\citep{Breschi2022}
% \end{rem}